{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DynaQ-GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgZhkPAQLmU3nVM5aaBYEu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/DynaQ_GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz-RUnO64NaM"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import random"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRWBOmgJ5KnN"
      },
      "source": [
        "# deterministic gridworld environment (with unreachable/blocked states) and deterministic greedy policy\n",
        "class GridWorld():\n",
        "    def __init__(self, seed=0, grid_h=4, grid_w=4, start_state=[0,0], goal_states=[[3,3]], blocked_states=[], episodes=50, alpha=0.4, gamma=0.9, epsilon_decay=0.9, N=20, reward_per_step=-1):\n",
        "        np.random.seed(seed)\n",
        "        self.grid_w = grid_w    # grid width\n",
        "        self.grid_h = grid_h    # grid height       \n",
        "        self.start_state = start_state  # start state, should have only one start state\n",
        "        self.goal_states = goal_states  # target/goal states\n",
        "        self.blocked_states = blocked_states # blocked states that cannot be visited\n",
        "        self.episodes = episodes    # number of episodes to run for\n",
        "        self.alpha = alpha  # equivalent of learning rate\n",
        "        self.gamma = gamma  # gamma - weight for future rewards\n",
        "        self.epsilon = 1    # epsilon greedy exploration hyper-parameter, always start with 1\n",
        "        self.epsilon_decay = epsilon_decay  # epsilon is reduced by this factor\n",
        "        self.N = N  # number of simulated experiences to run per episode\n",
        "        self.reward_per_step = reward_per_step  \n",
        "        self.directions = [[-1,0],[1,0],[0,-1],[0,1]] # up, down, left, right\n",
        "        self.num_actions = len(self.directions)\n",
        "        self.q_values = np.zeros((self.grid_h, self.grid_w, self.num_actions))  # state-action values initialized to 0\n",
        "        self.env_model = dict() # null initial model of the environment\n",
        "\n",
        "    # reset to initial values\n",
        "    def reset(self):\n",
        "        self.q_values = np.zeros((self.grid_h, self.grid_w, self.num_actions))\n",
        "        self.epsilon = 1\n",
        "        self.env_model = dict()\n",
        "\n",
        "    # display gridworld environment\n",
        "    def display_gridworld(self):\n",
        "        for i in range(self.grid_h):\n",
        "            for j in range(self.grid_w):\n",
        "                if [i,j] == self.start_state:\n",
        "                    print('[S]', end=' ')\n",
        "                elif [i,j] in self.goal_states: \n",
        "                    print('[G]', end=' ')\n",
        "                elif [i,j] in self.blocked_states:\n",
        "                    print('[X]', end=' ')\n",
        "                else:\n",
        "                    print('[ ]', end=' ')\n",
        "            print('')\n",
        "\n",
        "    # display state-action values for each action in each state\n",
        "    def display_q_values(self):\n",
        "        print('[')\n",
        "        for i in range(self.grid_h):\n",
        "            print('[', end='')\n",
        "            for j in range(self.grid_w):\n",
        "                print('[', end='')\n",
        "                for a in range(self.num_actions):\n",
        "                    print(self.q_values[i][j][a], sep='', end=', ')\n",
        "                print(\"]\", end=' ')\n",
        "            print(']')\n",
        "        print(']')\n",
        "\n",
        "    # display deterministic policy based in state-action values\n",
        "    def display_policy(self):        \n",
        "        for i in range(self.grid_h):            \n",
        "            for j in range(self.grid_w):\n",
        "                l = []      \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"[G]\", end=' ')\n",
        "                    continue\n",
        "                elif [i,j] in self.blocked_states:\n",
        "                    print(\"[X]\", end=' ')\n",
        "                    continue\n",
        "                for a in range(self.num_actions):\n",
        "                    l.append([self.q_values[i][j][a], a])\n",
        "                l.sort()\n",
        "                l.reverse()\n",
        "                m = [l[0][1]]\n",
        "                moves = [\"^\", \"v\", \"<\", \">\"]\n",
        "\n",
        "                for k in range(1, self.num_actions):\n",
        "                    if l[k][0] == l[0][0]:\n",
        "                        m.append(l[k][1])\n",
        "                    else: break\n",
        "                \n",
        "                print('[', end='')\n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end='&')\n",
        "                    else: print(moves[m[k]], end='] ')\n",
        "\n",
        "            print('')\n",
        "        print('')\n",
        "\n",
        "    # get next state from current state, action\n",
        "    def get_next_state(self, state, action):\n",
        "        next_state = copy.deepcopy(state)\n",
        "\n",
        "        next_state[0] += self.directions[action][0]\n",
        "        next_state[1] += self.directions[action][1]\n",
        "\n",
        "        if next_state[0]<0: next_state[0] = 0\n",
        "        elif next_state[0]>=self.grid_h: next_state[0] = self.grid_h-1\n",
        "\n",
        "        if next_state[1]<0: next_state[1] = 0\n",
        "        elif next_state[1]>=self.grid_w: next_state[1] = self.grid_w-1\n",
        "\n",
        "        if next_state in self.blocked_states: next_state = copy.deepcopy(state)\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    # q learning on gridworld\n",
        "    def q_learning(self):\n",
        "        self.reset()\n",
        "        for _ in range(self.episodes):\n",
        "\n",
        "            # pick initial state randomly which are not either goal or blocked state\n",
        "            k = np.random.choice(range(self.grid_h*self.grid_w))\n",
        "            i = k//self.grid_w\n",
        "            j = k - self.grid_w*i\n",
        "\n",
        "            while [i,j] in self.blocked_states or [i,j] in self.goal_states:\n",
        "                k = np.random.choice(self.grid_h*self.grid_w)\n",
        "                i = k//self.grid_w\n",
        "                j = k - self.grid_w*i\n",
        "\n",
        "            state = [i,j]                        \n",
        "\n",
        "            # run till terminal state is reached\n",
        "            while state not in self.goal_states:\n",
        "                # assign probability to pick action epsilon-greedily based on state-action values\n",
        "                prob = np.zeros(self.num_actions) + self.epsilon/self.num_actions\n",
        "                prob[np.argmax(self.q_values[state[0]][state[1]])] = 1 - self.epsilon + self.epsilon/self.num_actions\n",
        "                action = np.random.choice(range(self.num_actions), p=prob)\n",
        "                next_state = self.get_next_state(state, action)\n",
        "\n",
        "                # q learning update\n",
        "                self.q_values[state[0]][state[1]][action] += self.alpha * (self.reward_per_step + self.gamma * np.max(self.q_values[next_state[0]][next_state[1]]) - self.q_values[state[0]][state[1]][action])\n",
        "                \n",
        "                state = next_state\n",
        "\n",
        "            if (_+1)%(self.episodes//10) == 0:\n",
        "                print(\"episode %d:\"%(_+1))\n",
        "                self.display_q_values()            \n",
        "                self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    # dynaq on current environment\n",
        "    def dynaq(self):\n",
        "        self.reset()\n",
        "        # initialize number of visits to each state to 0\n",
        "        number_of_visits = dict()\n",
        "        \n",
        "        for _ in range(self.episodes):            \n",
        "            # pick initial state randomly which are not either goal or blocked state\n",
        "            k = np.random.choice(range(self.grid_h*self.grid_w))\n",
        "            i = k//self.grid_w\n",
        "            j = k - self.grid_w*i\n",
        "\n",
        "            while [i,j] in self.blocked_states or [i,j] in self.goal_states:\n",
        "                k = np.random.choice(self.grid_h*self.grid_w)\n",
        "                i = k//self.grid_w\n",
        "                j = k - self.grid_w*i\n",
        "            \n",
        "            state = [i,j]\n",
        "            \n",
        "            path = []\n",
        "\n",
        "            # run till terminal state is reached\n",
        "            while state not in self.goal_states:\n",
        "                # assign probability to pick action epsilon-greedily based on state-action values\n",
        "                prob = np.zeros(self.num_actions) + self.epsilon/self.num_actions\n",
        "                prob[np.argmax(self.q_values[state[0]][state[1]])] = 1 - self.epsilon + self.epsilon/self.num_actions\n",
        "                action = np.random.choice(self.num_actions, p=prob)\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                path.append((state, action))\n",
        "\n",
        "                # build model of deterministic environment based on experience\n",
        "                if (tuple(state), action) not in self.env_model:\n",
        "                    self.env_model[(tuple(state), action)] = (self.reward_per_step, next_state)\n",
        "                    number_of_visits[(tuple(state), action)] = 1\n",
        "                else:\n",
        "                    temp = self.env_model[(tuple(state), action)]                    \n",
        "                    self.env_model[(tuple(state), action)] = ((temp[0]*number_of_visits[(tuple(state), action)]+self.reward_per_step)/(number_of_visits[(tuple(state), action)]+1), next_state)\n",
        "                    number_of_visits[(tuple(state), action)] += 1\n",
        "\n",
        "                # q learning update\n",
        "                self.q_values[state[0]][state[1]][action] += self.alpha * (self.reward_per_step + self.gamma * np.max(self.q_values[next_state[0]][next_state[1]]) - self.q_values[state[0]][state[1]][action])\n",
        "                \n",
        "                state = next_state\n",
        "            \n",
        "            # learn from simulated experience N times\n",
        "            for n in range(self.N):\n",
        "                [(state, action)] = random.sample(path, 1)\n",
        "                (reward, next_state) = self.env_model[(tuple(state), action)]\n",
        "\n",
        "                # q learning update using simulated experience\n",
        "                self.q_values[state[0]][state[1]][action] += self.alpha * (reward + self.gamma * np.max(self.q_values[next_state[0]][next_state[1]]) - self.q_values[state[0]][state[1]][action])\n",
        "\n",
        "            if (_+1)%(self.episodes//2) == 0:\n",
        "                print(\"episode %d:\"%(_+1))\n",
        "                self.display_q_values()            \n",
        "                self.epsilon *= self.epsilon_decay\n",
        "        \n",
        "        print(\"\\ngenerated policy is-\")\n",
        "        self.display_policy()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxEyukwK5Kps",
        "outputId": "d48de47d-c1d1-4b2b-88c7-9be7fe458116"
      },
      "source": [
        "grid_h = 6\n",
        "grid_w = 9\n",
        "blocked_states = [[1,2],[2,2],[3,2],[4,5],[0,7],[1,7],[2,7]]\n",
        "goal_states = [[0,8]]\n",
        "grid = GridWorld(seed=0, grid_h=grid_h, grid_w=grid_w, start_state=[2,0], goal_states=goal_states, blocked_states=blocked_states, episodes=50, alpha=0.4, gamma=0.9, epsilon_decay=0.9, N=20, reward_per_step=-1)\n",
        "grid.display_gridworld()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ ] [ ] [ ] [ ] [ ] [ ] [ ] [X] [G] \n",
            "[ ] [ ] [X] [ ] [ ] [ ] [ ] [X] [ ] \n",
            "[S] [ ] [X] [ ] [ ] [ ] [ ] [X] [ ] \n",
            "[ ] [ ] [X] [ ] [ ] [ ] [ ] [ ] [ ] \n",
            "[ ] [ ] [ ] [ ] [ ] [X] [ ] [ ] [ ] \n",
            "[ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hSMDopHaQUk",
        "outputId": "2f1148f5-34e3-489c-90dc-3d9cbab9b31b"
      },
      "source": [
        "grid.dynaq()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 25:\n",
            "[\n",
            "[[-7.936685550774175, -8.14264036540527, -7.936708980508157, -7.709053760603241, ] [-7.70955579711994, -7.937391692391289, -7.936450518311378, -7.456312557048685, ] [-7.456453255994408, -7.456801063933587, -7.709778008470832, -7.175100988450424, ] [-7.175214945874221, -6.861624190289208, -7.457173257655629, -6.861667384506117, ] [-6.861668273518995, -6.513110165101468, -7.17528644929358, -6.513178737790515, ] [-6.513183100441505, -6.125776990632821, -6.861519965723206, -6.125780076367637, ] [-6.125783789049729, -5.695316402833531, -6.513182921833128, -6.125783140969413, ] [0.0, 0.0, 0.0, 0.0, ] [0.0, 0.0, 0.0, 0.0, ] ]\n",
            "[[-7.937853216631923, -7.94007112028268, -8.143015643984302, -7.9379835585294325, ] [-7.7093881576552885, -7.712069697088226, -8.143565288808711, -7.937821553290069, ] [0.0, 0.0, 0.0, 0.0, ] [-7.175125847173596, -6.5131665532442655, -6.861780396577413, -6.513183282719984, ] [-6.8616535210839436, -6.125773861004935, -6.861770455325234, -6.12577903827262, ] [-6.513188663732189, -5.695312281086243, -6.513181003054335, -5.695316383569981, ] [-6.125784107550189, -5.217019755569774, -6.125779366773839, -5.695315050661075, ] [0.0, 0.0, 0.0, 0.0, ] [-0.9999998673556482, -2.7080570748553616, -1.8998468365715084, -1.8982589762853679, ] ]\n",
            "[[-8.139558727337018, -7.712242407310488, -7.940607363353968, -7.712008979563228, ] [-7.938181867173331, -7.457956454947665, -7.940682859851092, -7.712043243871566, ] [0.0, 0.0, 0.0, 0.0, ] [-6.861684564124041, -6.125775239169126, -6.513190702126898, -6.125773082369981, ] [-6.5131932505810655, -5.695307105482538, -6.513187278054331, -5.6953063328506905, ] [-6.1257805251540365, -5.217016338409505, -6.125771907108652, -5.217017668223542, ] [-5.695313324993053, -4.685580420769655, -5.695312100392699, -5.217017614172664, ] [0.0, 0.0, 0.0, 0.0, ] [-1.8999995765932605, -3.438996591976402, -2.709998282125773, -2.7099968568144095, ] ]\n",
            "[[-7.9406077745865025, -7.458100720585831, -7.712144982510118, -7.458101347889942, ] [-7.712115898382169, -7.1756771973987385, -7.712198746086598, -7.458061955384553, ] [0.0, 0.0, 0.0, 0.0, ] [-6.513188749511769, -6.513190535057664, -6.125774838164256, -5.695307807855937, ] [-6.125774231370381, -6.125775780663036, -6.125776318276068, -5.217012112748564, ] [-5.695311601213425, -5.217016939715892, -5.695307396851524, -4.685574617366551, ] [-5.217018826941758, -5.217006454387384, -5.217016419345981, -4.0950934884591375, ] [-4.095093012550734, -4.685579816765196, -4.685574804404884, -3.4389949474578, ] [-2.709998852997895, -4.095093526479006, -4.095091732673927, -3.438997393964441, ] ]\n",
            "[[-7.7122331509848525, -7.7121484673315726, -7.458104765219131, -7.175681632471398, ] [-7.458099195946214, -7.458056283708189, -7.458082998071454, -6.861870621703369, ] [-6.861867663471686, -7.175611624465825, -7.175679480790779, -6.5131915592010365, ] [-6.125776717887612, -6.861816238441203, -6.861869715236161, -6.125770021410401, ] [-5.695307280603052, -6.513162928010713, -6.513186066110736, -6.125775476022014, ] [0.0, 0.0, 0.0, 0.0, ] [-4.685581015041685, -5.695300038287977, -5.216999714682242, -4.6855732381577395, ] [-4.095089475731825, -5.217012914015868, -5.217002054713312, -4.095095541729408, ] [-3.438996635328184, -4.685583018535617, -4.68557692117802, -4.095095271712855, ] ]\n",
            "[[-7.4581033889868715, -7.712198072196667, -7.71220248455124, -7.458008824955825, ] [-7.175676726307676, -7.458042299227907, -7.7121762975762005, -7.175618705439198, ] [-6.861862455576456, -7.175615596436143, -7.458041477743663, -6.861824542762314, ] [-6.513192438473617, -6.861815778255565, -7.175603419779978, -6.513155377540056, ] [-6.12577294980057, -6.513145783173554, -6.86178960409527, -6.125740526793714, ] [-6.125729557102665, -6.12569700029978, -6.513115080346995, -5.695292921055819, ] [-5.217014302963665, -5.695292016965552, -6.1257374576331065, -5.2170187814460105, ] [-4.685579288407215, -5.217005109425676, -5.695302883737648, -4.6855837110919305, ] [-4.095095119968164, -4.685579829853928, -5.217016367976773, -4.68557800260198, ] ]\n",
            "]\n",
            "episode 50:\n",
            "[\n",
            "[[-7.936685550774175, -8.143211377230655, -7.938628639527735, -7.7108608348708545, ] [-7.71089740444095, -7.938743025021335, -7.939273691368518, -7.457664891917086, ] [-7.4578579215080065, -7.457982271441131, -7.710139325620026, -7.175660230342528, ] [-7.175683125546296, -6.861883566706218, -7.457889312714426, -6.861881144685846, ] [-6.861847312792291, -6.513204167380104, -7.175681170974162, -6.513212175681952, ] [-6.513210077109644, -6.1257927633298594, -6.861882163104478, -6.125791869107025, ] [-6.125790965962883, -5.695327214697046, -6.51320722531767, -6.125794285884498, ] [0.0, 0.0, 0.0, 0.0, ] [0.0, 0.0, 0.0, 0.0, ] ]\n",
            "[[-7.937853216631923, -7.94007112028268, -8.143015643984302, -7.9392433983788315, ] [-7.711516317268365, -7.712069697088226, -8.143565288808711, -7.937821553290069, ] [0.0, 0.0, 0.0, 0.0, ] [-7.175664140689718, -6.5132125840525505, -6.8618807087744935, -6.513209279248404, ] [-6.861876338148661, -6.125792900706014, -6.861883591146896, -6.125790501951047, ] [-6.513212067950839, -5.69532565880102, -6.513210417740481, -5.695327512695011, ] [-6.1257932325002855, -5.217030931541779, -6.125792040668739, -5.69532710243339, ] [0.0, 0.0, 0.0, 0.0, ] [-0.9999999999999999, -2.7099998021038094, -1.8999999987851297, -1.899999181106151, ] ]\n",
            "[[-8.139558727337018, -7.712261703797192, -7.940607363353968, -7.712008979563228, ] [-7.938181867173331, -7.457956454947665, -7.940682859851092, -7.712043243871566, ] [0.0, 0.0, 0.0, 0.0, ] [-6.861872700380414, -6.125794479784973, -6.513207988686895, -6.125794338329421, ] [-6.513209908917282, -5.6953278693698985, -6.51321179533379, -5.695327348068203, ] [-6.12578918106247, -5.217030991375814, -6.125792961579617, -5.2170309794194125, ] [-5.695327657448293, -4.685589998610692, -5.695327780587712, -5.217030268776927, ] [0.0, 0.0, 0.0, 0.0, ] [-1.8999999999999997, -3.438999999994848, -2.7099999999999604, -2.7099999996659743, ] ]\n",
            "[[-7.94073597107956, -7.4581095601432965, -7.712238434590561, -7.458101347889942, ] [-7.712115898382169, -7.175682166634707, -7.712198746086598, -7.458061955384553, ] [0.0, 0.0, 0.0, 0.0, ] [-6.513211977904617, -6.513213037232086, -6.1257945116358234, -5.695327756268842, ] [-6.125794147714351, -6.125794940549162, -6.125793989441442, -5.21703099966094, ] [-5.695327495137558, -5.217030140493212, -5.6953278964430085, -4.6855899999774735, ] [-5.217030973136897, -5.217030995998925, -5.217030999119381, -4.095099999994926, ] [-4.095099999916854, -4.685589997002373, -4.685589996761167, -3.4389999999999974, ] [-2.7099999999999995, -4.095099999986266, -4.095099998948065, -3.4389999999887575, ] ]\n",
            "[[-7.7122563758312905, -7.7121934448452025, -7.458104765219131, -7.175684188528909, ] [-7.458107565167259, -7.458069869023442, -7.458082998071454, -6.861872831731975, ] [-6.861877348331195, -7.175678186556661, -7.1756848384357195, -6.513212418458545, ] [-6.125794485998638, -6.861883779337149, -6.8618889313553435, -6.12579492816591, ] [-5.695327895263466, -6.513214865398872, -6.51321443738311, -6.1257949892793295, ] [0.0, 0.0, 0.0, 0.0, ] [-4.685589999805108, -5.695327750322891, -5.21703086053033, -4.6855899993175125, ] [-4.095099999995262, -5.2170309834780575, -5.217030983923796, -4.0950999999984266, ] [-3.4389999999997993, -4.685589999781779, -4.685589998651049, -4.095099999871563, ] ]\n",
            "[[-7.458111717161027, -7.712208933733379, -7.712204667714841, -7.458028028931606, ] [-7.1756797161449875, -7.458048113494856, -7.712195868961099, -7.175665282529116, ] [-6.861879179267989, -7.175626193256119, -7.4580618391581694, -6.861885076662618, ] [-6.513213871272723, -6.861886112995352, -7.175649494691292, -6.513213771360449, ] [-6.1257950138676, -6.513214328030048, -6.86188454324245, -6.125795026108214, ] [-6.125794700014302, -6.1257933600619845, -6.513215301514517, -5.695327833132537, ] [-5.21703098585272, -5.6953272750995065, -6.125794641246405, -5.217030990262831, ] [-4.685589999335302, -5.217030958004773, -5.695327815314529, -4.68558999622494, ] [-4.095099999994311, -4.685589998764856, -5.2170309446203715, -4.685589999535579, ] ]\n",
            "]\n",
            "\n",
            "generated policy is-\n",
            "[>] [>] [>] [>] [v] [>] [v] [X] [G] \n",
            "[^] [^] [X] [>] [>] [v] [v] [X] [^] \n",
            "[>] [v] [X] [>] [>] [>] [v] [X] [^] \n",
            "[>] [v] [X] [>] [>] [>] [>] [>] [^] \n",
            "[>] [>] [>] [^] [^] [X] [>] [^] [^] \n",
            "[>] [>] [^] [>] [^] [>] [^] [>] [^] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ADkFcuITboaQ",
        "8PFmWnDXbz-Q",
        "uPfi6QTWWKqH",
        "x2WArJLPDRDw",
        "3sw7IhLpiP2J"
      ],
      "authorship_tag": "ABX9TyM6eS46/9VvI2Dm2zIOxHXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPTWDQ1btQo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIncFsMcHDI"
      },
      "source": [
        "## Generalized policy iteration with DP, Monte Carlo, TD learning and Q learning\n",
        "#### where the initial policy is a uniform one, equal probabilites of going to each neighbouring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCe0vwSXOv6"
      },
      "source": [
        "class gridworld():\n",
        "    def __init__(self, size = 4, reward = 0, penalty_per_step = -1, episodes = 100, alpha = 0.9, gamma = 0.99, epsilon_decay_factor = 0.99, seed = 0):\n",
        "        self.size = int(size) # square grid world size\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.state_values = np.zeros((size, size), dtype=np.float) # reward for all states initialised to 0   (state values for each state)     \n",
        "        self.goal_states = [[0,0],[size-1, size-1]] # goal states are the top left and bottom right corners of the grid\n",
        "        self.reward = reward # 0 reward on reaching goal state\n",
        "        self.epsilon = 1\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # terminate on reaching goal state OR stuck in infinite loop with 0 rewards per step in this loop\n",
        "\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "\n",
        "        self.action_values = np.zeros((self.size, self.size, 4)) # state action value matrix\n",
        "\n",
        "        self.penalty_per_step = penalty_per_step # -1 reward per movement \n",
        "        self.movements = [[-1,0], [1,0], [0,-1], [0,1]]\n",
        "        self.episodes = int(episodes) # number of episodes to run policy evalution\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def reset_grid(self): # reset all agent and environment variables\n",
        "        self.state_values = np.zeros((self.size, self.size))\n",
        "        self.epsilon = 1\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "        self.action_values = np.zeros((self.size, self.size, 4)) # reset state action value matrix\n",
        "\n",
        "    def greedy_policy_update_dp(self, curr_state):  # update policy greedily, second step of generalized policy iteration (GPI)\n",
        "        if curr_state not in self.goal_states:\n",
        "            self.policy[curr_state[0]][curr_state[1]] = np.zeros(4)\n",
        "            \n",
        "            up = self.next_state(curr_state, 0)\n",
        "            up = self.state_values[up[0]][up[1]]\n",
        "\n",
        "            down = self.next_state(curr_state, 1)\n",
        "            down = self.state_values[down[0]][down[1]]\n",
        "            \n",
        "            left = self.next_state(curr_state, 2)\n",
        "            left = self.state_values[left[0]][left[1]]\n",
        "            \n",
        "            right = self.next_state(curr_state, 3)\n",
        "            right = self.state_values[right[0]][right[1]]\n",
        "\n",
        "            arr = [[up, 0], [down, 1], [left, 2], [right, 3]]            \n",
        "            arr.sort()       \n",
        "            arr.reverse()     \n",
        "\n",
        "            m = [arr[0][1]]\n",
        "            for i in range(1, 4):\n",
        "                if arr[i][0] != arr[i-1][0]: break\n",
        "                else: m.append(arr[i][1])\n",
        "            \n",
        "            value = 1/len(m)\n",
        "            \n",
        "            for i in m:\n",
        "                self.policy[curr_state[0]][curr_state[1]][i] = value\n",
        "            \n",
        "            # print(self.policy[curr_state[0]][curr_state[1]])\n",
        "\n",
        "\n",
        "    def next_state(self, curr_state, direction): # direction = 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        change = self.movements[direction]\n",
        "        new_state = [curr_state[0]+change[0], curr_state[1]+change[1]]        \n",
        "\n",
        "        if(new_state[0]<0): new_state[0] = 0\n",
        "        elif(new_state[0]>self.size-1): new_state[0] = self.size-1\n",
        "        \n",
        "        if(new_state[1]<0): new_state[1] = 0\n",
        "        elif(new_state[1]>self.size-1): new_state[1] = self.size-1\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    def display_grid(self): # display the state of the grid world                \n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.state_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_action_values(self): # display state action values\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.action_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_policy(self): # display policy at each state, for each of the 4 actions\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):                \n",
        "                \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"stay\", end=\", \")\n",
        "                    continue\n",
        "\n",
        "                probs = [[self.policy[i][j][0], 0], [self.policy[i][j][1], 1], [self.policy[i][j][2], 2], [self.policy[i][j][3], 3]]\n",
        "                probs.sort()\n",
        "                probs.reverse()\n",
        "\n",
        "                moves = [\"up\", \"down\", \"left\", \"right\"]\n",
        "                m = [probs[0][1]]\n",
        "\n",
        "                for k in range(1,4):\n",
        "                    if probs[k][0] != probs[k-1][0]: break\n",
        "                    else: m.append(probs[k][1])\n",
        "                \n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end=\"&\")\n",
        "                    else: print(moves[m[k]], end=\", \")\n",
        "                \n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def evaluate_current_policy_dp(self): # policy evaluation using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.episodes):                        \n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step + self.gamma * self.state_values[new_state[0], new_state[1]]) * self.policy[i][j][k]                                \n",
        "                    \n",
        "                    self.state_values[i][j] = value\n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                \n",
        "    \n",
        "    def gpi_dp(self): # GPI using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # either update in self.state_values as you traverse OR in each epoch maintain a copy of self.state_values and use it for update at the end\n",
        "            state_values_copy = np.copy(self.state_values)\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.gamma * self.state_values[new_state[0]][new_state[1]]) * self.policy[i][j][k]                    \n",
        "                    state_values_copy[i,j] = value\n",
        "\n",
        "            if (self.state_values == state_values_copy).all():                 \n",
        "                print(\"Policy did not improve. Stopping...\")\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "                break\n",
        "            else: \n",
        "                self.state_values = state_values_copy\n",
        "            \n",
        "            for i in range(self.size): \n",
        "                for j in range(self.size): \n",
        "                    self.greedy_policy_update_dp([i,j])            \n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "\n",
        "    def evaluate_current_policy_monte_carlo(self): # evaluate policy using first visit monte carlo\n",
        "        self.reset_grid() # start from scratch\n",
        "\n",
        "        number_of_visits = np.zeros((self.size, self.size))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []\n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]] == 0:\n",
        "                    number_of_visits[state[0],state[1]] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]] = 1\n",
        "                \n",
        "                path.append(state)\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                direction = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                new_state = self.next_state(state, direction)            \n",
        "                state = new_state\n",
        "                \n",
        "            state_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_returns[i] = state_reward[i] + self.gamma * state_returns[i+1]\n",
        "            \n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            for p in range(len(path)):\n",
        "                if is_visited_this_episode[path[p][0]][path[p][1]] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[path[p][0]][path[p][1]] = 1\n",
        "                \n",
        "                self.state_values[path[p][0]][path[p][1]] += (state_returns[p]-self.state_values[path[p][0]][path[p][1]])/number_of_visits[path[p][0]][path[p][1]]\n",
        "            \n",
        "            if (_%(self.episodes/10) == 0):\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                # print(\"policy - \")\n",
        "                # self.display_policy()\n",
        "    \n",
        "    def epsilon_greedy_policy_update_monte_carlo(self): # epsilon greedy update of policy, second step of monte carlo policy improvement\n",
        "        \n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "\n",
        "                if [i,j] in self.goal_states: continue\n",
        "\n",
        "                prob = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)        \n",
        "                prob[np.argmax(self.action_values[i][j])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "\n",
        "                self.policy[i][j] = prob\n",
        "            \n",
        "                # print(\"updated policy at\", curr_state, \"to\", self.policy[i][j])\n",
        "\n",
        "    def gpi_monte_carlo(self): # GPI using epsilon greedy first visit monte carlo\n",
        "        self.reset_grid()\n",
        "        number_of_visits = np.zeros((self.size, self.size, 4))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            \n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            \n",
        "            k = np.random.choice(range(1, self.size*self.size-1))            \n",
        "            i = k//4\n",
        "            j = k - 4*i                                    \n",
        "            \n",
        "            action = np.random.choice([0,1,2,3])\n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []            \n",
        "\n",
        "            while True:\n",
        "                if is_visited_this_episode[state[0]][state[1]][action] == 0:\n",
        "                    number_of_visits[state[0]][state[1]][action] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]][action] = 1\n",
        "\n",
        "                # print(\"in state\", state, \"took action\", action, \"with policy\", self.policy[state[0]][state[1]])                             \n",
        "\n",
        "                path.append([state, action])\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "\n",
        "                # IMPORTANT STEP - first select next state based on current state and action, \n",
        "                # then select next action based on probabilities in this new state\n",
        "                state = self.next_state(state, action)                                \n",
        "\n",
        "                if state in self.goal_states: break\n",
        "        \n",
        "                action = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                \n",
        "            state_action_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_action_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_action_returns[i] = state_reward[i] + self.gamma * state_action_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1   \n",
        "\n",
        "                # self.action_values[x][y][a] += (state_action_returns[p]-self.action_values[x][y][a])/number_of_visits[x][y][a]\n",
        "                self.action_values[x][y][a] = np.mean(state_action_returns[p:])\n",
        "                        \n",
        "            self.epsilon_greedy_policy_update_monte_carlo() # improve the policy based on updated state-action values                                                        \n",
        "\n",
        "            if (_%(self.episodes/10) == 0):                \n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor   \n",
        "\n",
        "    def evaluate_current_policy_tdlearning(self): # evaluate policy using TD(0) learning\n",
        "        self.reset_grid()\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i\n",
        "            state = [i,j]            \n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                action = np.random.choice([0,1,2,3], p=self.policy[i][j]) # pick action based on current policy, state            \n",
        "                new_state = self.next_state(state, action)\n",
        "                self.state_values[state[0]][state[1]] += self.alpha * (self.penalty_per_step + self.gamma * self.state_values[new_state[0]][new_state[1]] - self.state_values[state[0]][state[1]])\n",
        "                state = new_state\n",
        "\n",
        "            if _%(self.episodes/10)==0:\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "\n",
        "    def sarsa(self): # on-policy TD learning GPI\n",
        "        self.reset_grid()\n",
        "\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            \n",
        "            k = np.random.choice(range(1,15))\n",
        "            i= k//4\n",
        "            j = k - 4*i\n",
        "            state = [i,j]\n",
        "\n",
        "            # selecting action using epsilon greedy policy at state\n",
        "            pr = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)\n",
        "            pr[np.argmax(self.action_values[state[0]][state[1]])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "            self.policy[state[0]][state[1]] = pr\n",
        "            action = np.random.choice([0,1,2,3], p=pr)\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                new_state = self.next_state(state, action)\n",
        "                # selecting next action using epsilon greedy policy at new_state\n",
        "                pr = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)\n",
        "                pr[np.argmax(self.action_values[new_state[0]][new_state[1]])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "                self.policy[new_state[0]][new_state[1]] = pr # updating policy at new_state epsilon greedily\n",
        "                new_action = np.random.choice([0,1,2,3], p=pr)\n",
        "\n",
        "                self.action_values[state[0]][state[1]][action] += self.alpha * (self.penalty_per_step + self.gamma * self.action_values[new_state[0]][new_state[1]][new_action] - self.action_values[state[0]][state[1]][action])\n",
        "                \n",
        "                state = new_state\n",
        "                action = new_action\n",
        "            \n",
        "            if _%(self.episodes/10)==0:\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor       \n",
        "\n",
        "    def qlearning(self): # off-policy TD learning\n",
        "        self.reset_grid()        \n",
        "        policy_update_count = self.episodes/10        \n",
        "\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            if policy_update_count == 0:                 \n",
        "                policy_update_count = self.episodes/10\n",
        "            \n",
        "            policy_update_count -= 1\n",
        "\n",
        "            k = np.random.choice(range(1,self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i\n",
        "            state = [i,j]            \n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                # select action in current state epsilon greedily\n",
        "                pr = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)\n",
        "                pr[np.argmax(self.action_values[i][j])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "                self.policy[i][j] = pr # update policy at current state\n",
        "                action = np.random.choice([0,1,2,3], p=pr)\n",
        "\n",
        "                new_state = self.next_state(state, action) # next state from current state and action\n",
        "\n",
        "                # update action value for current state, action pair by taking max (greedily) over action values of new_state (this becomes the target value)\n",
        "                self.action_values[state[0]][state[1]][action] += self.alpha * (self.penalty_per_step + self.gamma * np.max(self.action_values[new_state[0]][new_state[1]]) - self.action_values[state[0]][state[1]][action])\n",
        "                state = new_state\n",
        "            \n",
        "            if _%(self.episodes/10)==0:\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor  \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkFcuITboaQ"
      },
      "source": [
        "## Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gVHaw87EBs",
        "outputId": "cb8430de-6679-4eb7-ea42-2f28b30a3115"
      },
      "source": [
        "grid = gridworld(episodes=100, gamma = 0.9, seed = 0)\n",
        "# grid.evaluate_current_policy_dp()\n",
        "grid.gpi_dp()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy did not improve. Stopping...\n",
            "epoch #4 [\n",
            "       [ 0.0, -1.0, -1.9, -2.71, ]\n",
            "       [ -1.0, -1.9, -2.71, -1.9, ]\n",
            "       [ -1.9, -2.71, -1.9, -1.0, ]\n",
            "       [ -2.71, -1.9, -1.0, 0.0, ]\n",
            "]\n",
            "policy -\n",
            "[\n",
            "       [ stay, left, left, left&down, ]\n",
            "       [ up, left&up, right&left&down&up, down, ]\n",
            "       [ up, right&left&down&up, right&down, down, ]\n",
            "       [ right&up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmWnDXbz-Q"
      },
      "source": [
        "## Monte Carlo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI3VPo26ZBP4",
        "outputId": "168074e6-7bc8-457d-f001-d04d5166052a"
      },
      "source": [
        "grid = gridworld(size = 4, episodes=100, gamma = 0.8, epsilon_decay_factor = 0.5, seed = 0)\n",
        "# grid.evaluate_current_policy_monte_carlo()\n",
        "grid.gpi_monte_carlo()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ 0.0, -1.0, 0.0, 0.0, ]\n",
            "       [ -4.407095119842893, -3.808101121192415, 0.0, 0.0, ]\n",
            "       [ -4.446367306568033, -3.6384079861977523, -2.4926258176, -1.0, ]\n",
            "       [ -4.251987300034684, -4.250054426331155, -2.0069356013308197, 0.0, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ 0.0, -3.9293197550788266, -4.601437166247207, -4.949477169613706, ]\n",
            "       [ -3.947522471583098, -4.193485617650129, -4.533937734713759, -4.946706484359731, ]\n",
            "       [ -4.415114830578479, -4.01102157938329, -3.6428574751720197, -2.63521036244089, ]\n",
            "       [ -4.408674003501692, -4.380777322322286, -3.06094981755488, 0.0, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ 0.0, -4.080019386565007, -4.514692520695907, -4.961836924633047, ]\n",
            "       [ -3.8492170891980098, -4.307672151048828, -4.6477970671303135, -4.948882604618631, ]\n",
            "       [ -4.299468431056001, -4.20161687957188, -3.968415019798933, -2.9616083389209225, ]\n",
            "       [ -4.508985762918892, -4.490389328936819, -3.008555051464264, 0.0, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ 0.0, -4.052700213895632, -4.42125905102205, -4.790243979230169, ]\n",
            "       [ -3.7489483609424727, -4.150186894098453, -4.454185724846272, -4.814216668887388, ]\n",
            "       [ -4.299364329610695, -4.31255306508692, -3.9646022024637366, -3.398840528468098, ]\n",
            "       [ -4.392226055194314, -4.4171035136688355, -3.1237255870160623, 0.0, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ 0.0, -4.035467869146636, -4.532617536122994, -4.800022337040892, ]\n",
            "       [ -3.76045808125144, -4.182205259262917, -4.523598254760393, -4.760640147979186, ]\n",
            "       [ -4.24926075761077, -4.318682585942697, -3.9632001140602355, -3.384387245524823, ]\n",
            "       [ -4.422070380996887, -4.238697546465013, -2.945275956695077, 0.0, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ 0.0, -3.9099065536993383, -4.482231625074508, -4.748980588639893, ]\n",
            "       [ -3.7978954043760322, -4.104938040298183, -4.497880816881052, -4.701535540181763, ]\n",
            "       [ -4.243502132984537, -4.361169687365901, -3.861681658839727, -3.4758384969018516, ]\n",
            "       [ -4.346313231827578, -4.213548084598056, -2.8698412570382517, 0.0, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ 0.0, -3.813981792099114, -4.541418972901883, -4.7107688633114515, ]\n",
            "       [ -3.71764762841019, -4.185720510022753, -4.602179467282952, -4.742889445115133, ]\n",
            "       [ -4.210441339427358, -4.440092736417489, -4.034678194342107, -3.9057586496434875, ]\n",
            "       [ -4.405754353248604, -4.295223456658515, -3.053786794849428, 0.0, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ 0.0, -3.8836310559664975, -4.532066230651834, -4.699526244705008, ]\n",
            "       [ -3.7688724116473287, -4.231805522910184, -4.594644501536869, -4.6499763866418915, ]\n",
            "       [ -4.205468291980692, -4.494393918518598, -4.086072633571363, -3.6683355974519425, ]\n",
            "       [ -4.4619607351801935, -4.320751099995259, -3.03382287008124, 0.0, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ 0.0, -3.902469072144679, -4.541887817478309, -4.711976731001692, ]\n",
            "       [ -3.703457070082439, -4.212039959193038, -4.630815369206201, -4.599135171048834, ]\n",
            "       [ -4.211904660371375, -4.460759949489107, -4.135669834197998, -3.694282579147083, ]\n",
            "       [ -4.4586016692663835, -4.342043466805112, -3.0437298500020917, 0.0, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ 0.0, -3.822115906456238, -4.567423418423555, -4.698148016635028, ]\n",
            "       [ -3.6837295848701324, -4.224839305363805, -4.618141058615165, -4.619704370063106, ]\n",
            "       [ -4.240828381293138, -4.468306593140569, -4.122913849326781, -3.7072171311085174, ]\n",
            "       [ -4.490068585259431, -4.394890839965273, -3.054462218793125, 0.0, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEln2xQmrhP3"
      },
      "source": [
        "##TD Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPfi6QTWWKqH"
      },
      "source": [
        "### TD(0) policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb-E9O6Mrkw5",
        "outputId": "ca93b101-112b-4e3f-d024-bf3b8e33adbb"
      },
      "source": [
        "grid = gridworld(size=4, episodes=100, alpha = 0.4, gamma=0.9, seed=0)\n",
        "grid.evaluate_current_policy_tdlearning()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ 0.0, -0.4, 0.0, 0.0, ]\n",
            "       [ -2.9629724360937026, -2.639936981081504, 0.0, 0.0, ]\n",
            "       [ -5.44151317331102, -4.014618934682042, -1.7166933635267365, -0.4, ]\n",
            "       [ -6.166773633673814, -4.727311671708557, -2.3115101357220333, 0.0, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ 0.0, -3.6906582799099343, -6.292470090417065, -6.500122623042976, ]\n",
            "       [ -3.5077005430554573, -6.31259152772494, -6.566089336250388, -6.33824278069204, ]\n",
            "       [ -5.998135981012131, -6.581771664356883, -4.915559162026243, -1.9986345158012115, ]\n",
            "       [ -7.317762953663052, -6.085004646856289, -5.213237152387899, 0.0, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ 0.0, -5.880178946660615, -7.35219747458223, -7.5045626184122245, ]\n",
            "       [ -2.6167744394189896, -6.340465641471173, -7.130925611995167, -5.888951729411248, ]\n",
            "       [ -7.250678064283298, -7.189584699331956, -6.797452462621809, -3.614429250769203, ]\n",
            "       [ -7.782966144868626, -6.818831956791831, -3.3627339164741805, 0.0, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ 0.0, -4.844181901626105, -7.744939139275797, -7.916228246988059, ]\n",
            "       [ -4.971699733080612, -6.601481055375683, -6.6487287336997305, -7.04679370988137, ]\n",
            "       [ -5.878372057001802, -6.534680201081627, -5.103215530887098, -6.349752806044159, ]\n",
            "       [ -7.057070427537211, -6.490203706545806, -4.458337665232639, 0.0, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ 0.0, -6.710815033211553, -6.894401369396651, -8.088713930154489, ]\n",
            "       [ -4.347789517666648, -7.0407421938079455, -6.251824054901412, -7.17719251424549, ]\n",
            "       [ -7.0583890972577015, -6.866725711942888, -6.2984287338515434, -4.6302872582576455, ]\n",
            "       [ -7.179597691693744, -5.46767808372083, -2.404787727487341, 0.0, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ 0.0, -3.827045651496853, -7.244194177185483, -7.733710366184942, ]\n",
            "       [ -6.995588873447186, -6.688051876181542, -7.761245796830943, -8.349785678719904, ]\n",
            "       [ -6.974032226363739, -6.705813592818238, -6.176286836889591, -7.160742030421436, ]\n",
            "       [ -6.773529340910786, -5.24381218909238, -2.165024017539532, 0.0, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ 0.0, -4.139017934134728, -8.377388514697152, -8.500467824571382, ]\n",
            "       [ -4.2899237368621765, -7.578056135707446, -8.354836146886889, -7.873098338513429, ]\n",
            "       [ -7.382300869027415, -7.36011258116839, -7.218113108992989, -4.632748954450218, ]\n",
            "       [ -7.588629616868737, -7.558267798086292, -4.61300454959362, 0.0, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ 0.0, -7.81234289503057, -8.21924239208209, -8.652202136435566, ]\n",
            "       [ -5.1284132629114225, -8.045525457874126, -7.004778526600443, -7.648640778607739, ]\n",
            "       [ -7.935571874491101, -6.967085818830014, -5.76630283503324, -4.405555669499254, ]\n",
            "       [ -7.640078336456421, -7.587839049200903, -5.2023310408183985, 0.0, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ 0.0, -4.246859805957811, -7.2521315713719074, -7.964689550753896, ]\n",
            "       [ -3.170098147093424, -6.865766450554156, -8.265819371269552, -7.939036847994827, ]\n",
            "       [ -7.6913119102153065, -7.761320036094515, -7.742779644914741, -4.325674418466876, ]\n",
            "       [ -8.052872652759818, -7.227077115912948, -2.3700726413958995, 0.0, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ 0.0, -5.218895580386905, -6.982436247550111, -8.208336689709357, ]\n",
            "       [ -4.489311366194151, -7.083783749832388, -6.905078159758922, -7.5891630086025845, ]\n",
            "       [ -8.112200668965475, -7.338081729706917, -5.838704843853185, -3.3254142643790514, ]\n",
            "       [ -8.463378491859556, -7.226742085952345, -5.236440797853145, 0.0, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2WArJLPDRDw"
      },
      "source": [
        "### SARSA On-policy TD control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNJtF4USDQR5",
        "outputId": "d40122a3-56d4-4f44-dbe2-007a7fa3cd46"
      },
      "source": [
        "grid = gridworld(size=4, episodes=100, alpha=0.6, gamma=0.9, epsilon_decay_factor=0.9, seed=0)\n",
        "grid.sarsa()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode #10 [\n",
            "       [ [0. 0. 0. 0.], [-1.73753636 -2.90246222 -0.84       -1.66944106], [-1.0656     -2.43776907 -1.0536     -1.9622112 ], [-1.3783776  -1.805136   -1.88379072 -1.601664  ], ]\n",
            "       [ [ 0.         -3.36776692 -1.9369272  -2.47295943], [-2.36194128 -4.01072865 -2.64095664 -2.93739556], [-2.02884096 -2.98814258 -3.64958835 -3.21957975], [-0.84       -1.57104    -3.51638361 -0.936     ], ]\n",
            "       [ [-1.51672595 -3.12444722 -3.05327265 -3.53044584], [-2.71644937 -2.37115944 -2.97987341 -2.17875465], [-2.43209232 -0.6        -3.55071167 -1.77045504], [ 0.         -0.9744     -2.15298858 -0.6       ], ]\n",
            "       [ [-2.28182225 -2.71943528 -1.89348175 -2.5859292 ], [-2.9273422  -3.08449283 -0.6        -2.08330155], [-1.9373543  -0.924      -2.76609963 -0.9744    ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, stay, ]\n",
            "]\n",
            "episode #20 [\n",
            "       [ [0. 0. 0. 0.], [-2.33815747 -3.18710147 -0.9744     -2.21110056], [-2.30045528 -3.45528252 -2.32813571 -2.40876173], [-1.3783776  -2.17935936 -3.07541678 -2.25791259], ]\n",
            "       [ [-0.6        -3.72511316 -1.56828463 -2.47295943], [-3.51021492 -3.9485442  -2.70232334 -3.19386849], [-3.53441187 -2.42243731 -3.93176641 -3.21957975], [-1.92284698 -2.35784437 -3.46919067 -2.65571822], ]\n",
            "       [ [-3.02528452 -3.24618065 -3.68600365 -3.91417244], [-3.57420162 -3.2140899  -3.50514023 -2.42754758], [-3.86148622 -1.66176    -3.62941631 -1.87887834], [-2.79301566 -0.99973786 -2.67049647 -1.91809785], ]\n",
            "       [ [-2.28182225 -2.71943528 -1.89348175 -3.38279423], [-3.59595974 -3.41456192 -0.6        -2.47949194], [-2.7983087  -2.74745605 -2.83142269 -0.995904  ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, up, ]\n",
            "       [ up, left, down, up, ]\n",
            "       [ up, right, down, down, ]\n",
            "       [ left, left, right, stay, ]\n",
            "]\n",
            "episode #30 [\n",
            "       [ [0. 0. 0. 0.], [-2.06143899 -3.20040687 -0.98976    -2.21110056], [-2.82091344 -3.29022916 -3.25228908 -2.74240424], [-3.01899504 -3.3451067  -3.07639415 -2.68001909], ]\n",
            "       [ [-0.936      -3.7236989  -3.23887496 -3.75929467], [-3.51021492 -3.94166326 -3.19146302 -3.64386473], [-3.77423577 -2.86083677 -4.30492043 -3.21957975], [-1.92284698 -2.35784437 -4.11083013 -2.65571822], ]\n",
            "       [ [-2.31180476 -3.95490811 -3.68600365 -3.7480517 ], [-4.09273764 -3.40365245 -3.50514023 -3.15672708], [-3.86148622 -2.27963215 -3.93579156 -2.58681326], [-2.79301566 -0.99999329 -2.68279289 -2.27966553], ]\n",
            "       [ [-3.89388181 -4.45223958 -4.4013539  -3.79698113], [-3.77964717 -4.06569354 -3.80070812 -3.03207723], [-3.65615871 -2.74745605 -2.05656908 -0.99973786], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, down, ]\n",
            "       [ up, left, down, up, ]\n",
            "       [ up, down, down, down, ]\n",
            "       [ right, left, right, stay, ]\n",
            "]\n",
            "episode #40 [\n",
            "       [ [0. 0. 0. 0.], [-2.06143899 -3.20040687 -0.98976    -2.21110056], [-3.42621826 -3.29022916 -3.25228908 -3.48792612], [-3.79134701 -3.20744439 -3.35385092 -4.02045577], ]\n",
            "       [ [-0.9744     -3.99865999 -3.23887496 -4.07140482], [-3.51021492 -3.94166326 -3.19146302 -3.60239775], [-3.77423577 -3.18701359 -4.30492043 -3.21957975], [-3.86591163 -1.9921922  -3.85101353 -3.83891636], ]\n",
            "       [ [-2.72313074 -3.95490811 -4.25828395 -3.7480517 ], [-4.09273764 -4.09436982 -3.25043066 -3.15672708], [-3.86148622 -2.0517113  -3.93579156 -2.17472521], [-3.17554358 -0.99999993 -2.68279289 -2.34803424], ]\n",
            "       [ [-4.14799469 -4.75762694 -4.38884321 -4.33371908], [-4.0727082  -4.06569354 -4.51249323 -1.97242576], [-3.65615871 -2.74745605 -2.05656908 -0.99999329], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, left, ]\n",
            "       [ up, left, down, down, ]\n",
            "       [ left, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #50 [\n",
            "       [ [0. 0. 0. 0.], [-2.18521612 -3.82545753 -0.9983616  -3.89565414], [-3.59809735 -4.2407487  -3.95626111 -4.46086405], [-4.28758492 -4.43878311 -3.44534731 -4.34394072], ]\n",
            "       [ [-0.995904   -3.99865999 -4.09410859 -4.19952126], [-3.41811371 -3.94166326 -3.93096985 -4.28927201], [-4.39181892 -3.98931024 -3.66164191 -3.96737921], [-4.19369204 -3.26934802 -4.52641292 -3.21135033], ]\n",
            "       [ [-2.72313074 -3.95490811 -4.25828395 -3.7480517 ], [-4.09273764 -3.6410726  -3.25043066 -3.15672708], [-3.88316755 -2.1524925  -4.69854353 -3.08336789], [-4.01127937 -0.99999997 -2.84746877 -3.50996227], ]\n",
            "       [ [-4.14799469 -4.90727896 -4.59633158 -3.64903497], [-4.0727082  -4.06569354 -5.30018183 -3.44193673], [-4.14766604 -2.41304203 -3.8319751  -0.99999993], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, left, ]\n",
            "       [ up, up, right, right, ]\n",
            "       [ left, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #60 [\n",
            "       [ [0. 0. 0. 0.], [-2.18521612 -3.28696583 -0.99989514 -3.89565414], [-4.32926561 -4.45052701 -4.24825151 -4.46086405], [-4.28758492 -4.43878311 -3.9211115  -4.34394072], ]\n",
            "       [ [-0.99934464 -3.99865999 -4.09410859 -4.19952126], [-1.9967346  -3.94166326 -3.93096985 -4.45809357], [-4.64673187 -4.41671023 -3.66164191 -4.55742206], [-4.13796436 -2.60355225 -4.52641292 -4.40958816], ]\n",
            "       [ [-2.22704046 -3.95490811 -4.25828395 -3.7480517 ], [-4.55330194 -3.6410726  -3.37066286 -3.86117126], [-4.52484924 -2.14726816 -4.40973955 -4.0640067 ], [-4.23443082 -0.99999999 -4.11845977 -4.4905455 ], ]\n",
            "       [ [-4.14799469 -4.90727896 -4.59633158 -4.25887642], [-3.73438502 -4.06569354 -5.30018183 -4.26415448], [-3.70220278 -3.80495647 -3.99143587 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, down, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, left, down, down, ]\n",
            "       [ right, right, right, stay, ]\n",
            "]\n",
            "episode #70 [\n",
            "       [ [0. 0. 0. 0.], [-2.01408282 -3.38601374 -0.99999893 -4.45231747], [-4.32926561 -4.45052701 -2.83927796 -4.46086405], [-4.28758492 -4.43878311 -3.9211115  -4.34394072], ]\n",
            "       [ [-0.99973786 -3.99865999 -4.09410859 -4.19952126], [-2.32648036 -3.87813348 -3.93096985 -4.45809357], [-4.64673187 -4.41671023 -3.09899751 -4.55742206], [-4.13796436 -2.1814209  -4.3878518  -3.90392336], ]\n",
            "       [ [-2.03046229 -4.42188038 -4.3272615  -4.06539988], [-4.55330194 -4.35907246 -3.99599718 -3.86117126], [-4.52484924 -3.4542394  -4.40973955 -4.0640067 ], [-4.23443082 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -4.90727896 -4.59633158 -4.25887642], [-3.73438502 -4.06569354 -5.30018183 -4.30485129], [-3.16029104 -2.66198259 -3.99143587 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, down, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "episode #80 [\n",
            "       [ [0. 0. 0. 0.], [-2.01408282 -3.38601374 -1.         -4.45231747], [-4.32926561 -4.88944601 -1.92404543 -4.8843675 ], [-4.28758492 -4.43878311 -3.29633938 -4.34394072], ]\n",
            "       [ [-0.99989514 -3.99865999 -4.09410859 -4.19952126], [-1.96823681 -3.87813348 -3.93096985 -4.45809357], [-3.68757647 -3.86010954 -2.95771871 -4.55742206], [-4.13796436 -2.1814209  -4.16952435 -4.97209242], ]\n",
            "       [ [-1.95204336 -5.01868279 -4.3272615  -4.06539988], [-3.67762017 -4.35907246 -4.58621427 -4.00975778], [-4.52484924 -4.12304192 -4.40973955 -3.99283372], [-4.74051736 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -5.04493064 -4.73832589 -4.69546997], [-4.75942004 -5.08837561 -5.14954628 -4.30485129], [-3.71046625 -3.3713502  -4.21314226 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, right, right, down, ]\n",
            "       [ right, up, right, stay, ]\n",
            "]\n",
            "episode #90 [\n",
            "       [ [0. 0. 0. 0.], [-2.01408282 -3.69090625 -1.         -4.45231747], [-4.32926561 -4.0925835  -1.90153891 -4.8843675 ], [-4.79785992 -4.43878311 -3.39740111 -4.86190855], ]\n",
            "       [ [-0.99995806 -3.99865999 -4.09410859 -4.19952126], [-1.9134732  -3.87813348 -3.93096985 -4.45809357], [-3.68757647 -3.86010954 -2.80984619 -3.60093611], [-4.13796436 -2.01256836 -4.16952435 -4.97209242], ]\n",
            "       [ [-1.95204336 -5.01868279 -4.3272615  -4.06539988], [-3.11523689 -4.35907246 -4.58621427 -4.00975778], [-4.52484924 -4.12304192 -4.40973955 -3.99283372], [-4.74051736 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -5.04493064 -4.73832589 -4.69546997], [-4.75942004 -5.08837561 -5.14954628 -4.30485129], [-3.71046625 -3.3713502  -4.21314226 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ right, up, right, stay, ]\n",
            "]\n",
            "episode #100 [\n",
            "       [ [0. 0. 0. 0.], [-1.94563313 -3.69090625 -1.         -4.45231747], [-3.35803865 -4.0925835  -1.90024623 -4.8843675 ], [-4.79785992 -4.43878311 -2.98579145 -4.86190855], ]\n",
            "       [ [-0.99998322 -3.99865999 -2.77762079 -4.19952126], [-1.90215571 -3.83348131 -3.93096985 -4.45809357], [-3.68757647 -3.86010954 -3.81813056 -3.60093611], [-4.13796436 -2.01256836 -4.16952435 -4.97209242], ]\n",
            "       [ [-1.95204336 -5.01868279 -4.3272615  -4.06539988], [-2.87937028 -4.35907246 -4.58621427 -4.00975778], [-4.52484924 -2.78921677 -4.40973955 -3.99283372], [-4.74051736 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -5.04493064 -4.73832589 -4.69546997], [-4.75942004 -5.08837561 -5.14954628 -2.86194052], [-3.71046625 -3.3713502  -4.21314226 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ right, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUsRw3lQRxMe"
      },
      "source": [
        "### Off-policy Q-Learning TD control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzGI0U53R3BY",
        "outputId": "d1ce1131-2c17-4980-db16-c7f2a027ec77"
      },
      "source": [
        "grid = gridworld(size=4, episodes=40, alpha=0.5, gamma=0.9, epsilon_decay_factor=0.9, seed=0)\n",
        "grid.qlearning()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode #4 [\n",
            "       [ [0. 0. 0. 0.], [0. 0. 0. 0.], [0. 0. 0. 0.], [0. 0. 0. 0.], ]\n",
            "       [ [ 0.  -0.5 -0.5  0. ], [0. 0. 0. 0.], [0. 0. 0. 0.], [0. 0. 0. 0.], ]\n",
            "       [ [ 0.  -0.5  0.  -0.5], [ 0.    0.    0.   -0.75], [ 0.   -0.75  0.    0.  ], [0. 0. 0. 0.], ]\n",
            "       [ [ 0.    -0.75   0.    -0.875], [-0.5  -0.5  -0.75 -0.5 ], [ 0.      0.      0.     -0.9375], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, stay, ]\n",
            "]\n",
            "episode #8 [\n",
            "       [ [0. 0. 0. 0.], [ 0.   0.  -0.5 -0.5], [ 0.  -0.5  0.   0. ], [0. 0. 0. 0.], ]\n",
            "       [ [-0.5    -1.0875 -0.975  -1.275 ], [-0.75   -1.0875 -0.725  -0.5   ], [ 0.    -0.875  0.    -0.5  ], [ 0.  -0.5  0.  -0.5], ]\n",
            "       [ [-1.275      -1.895625   -1.61132812 -1.603125  ], [-0.75      -1.359375  -1.7590625 -1.1625   ], [-0.75 -0.75 -1.2  -0.75], [ 0.    -0.75  -0.725  0.   ], ]\n",
            "       [ [-1.65664063 -1.7675     -1.73625    -1.1625    ], [-1.275 -0.975 -1.68  -0.5  ], [ 0.      0.      0.     -0.9375], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ up, up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right, right&left&down&up, right&left&down&up, stay, ]\n",
            "]\n",
            "episode #12 [\n",
            "       [ [0. 0. 0. 0.], [-0.75       -0.82625    -0.75       -1.57242188], [-1.44257813 -1.475      -1.25625    -1.97132813], [-1.40625    -1.209375   -1.70273438 -1.42625   ], ]\n",
            "       [ [-0.75     -1.656875 -1.33125  -1.569375], [-1.30625  -1.566875 -1.26875  -0.75    ], [-0.875   -1.575   -0.82625 -1.1    ], [-1.19375   -0.875     -1.1218125 -1.19375  ], ]\n",
            "       [ [-1.40625    -2.12445313 -2.26306915 -1.99085938], [-1.53125    -1.55117188 -2.20230469 -1.1625    ], [-1.2125 -1.1    -1.7925 -0.875 ], [-0.5   -0.875 -1.2    0.   ], ]\n",
            "       [ [-2.22018066 -1.9765625  -1.89125    -1.30625   ], [-1.889375 -1.2125   -2.019375 -0.875   ], [-0.8375  -0.975   -1.4375  -0.96875], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right, ]\n",
            "       [ up, right&left&down&up, left, right&left&down&up, ]\n",
            "       [ up, up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right, right&left&down&up, up, stay, ]\n",
            "]\n",
            "episode #16 [\n",
            "       [ [0. 0. 0. 0.], [-0.75       -0.82625    -0.75       -1.57242188], [-1.44257813 -1.8674375  -1.25625    -2.14775391], [-1.40625    -1.64296875 -1.91667969 -2.09425781], ]\n",
            "       [ [-0.875      -2.31046875 -1.503125   -2.09984375], [-1.30625    -2.13734375 -1.76074219 -1.37      ], [-1.81671875 -1.575      -1.9359375  -1.6578125 ], [-1.7296875 -0.9375    -2.1460625 -1.6390625], ]\n",
            "       [ [-1.6078125  -2.66015039 -2.37128557 -2.49339404], [-2.01453125 -1.86705469 -2.41644043 -1.86125   ], [-1.4780625 -1.1       -2.2393125 -1.5171875], [-1.171875 -0.984375 -1.2      -1.390625], ]\n",
            "       [ [-2.40639801 -2.07609375 -2.74682312 -2.29980469], [-1.889375   -2.17940625 -2.84418457 -1.593125  ], [-1.3125    -1.4234375 -1.89375   -0.96875  ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, left, right, ]\n",
            "       [ up, right&left&down&up, left, right&left&down&up, ]\n",
            "       [ up, up, right, right, ]\n",
            "       [ right, right, up, stay, ]\n",
            "]\n",
            "episode #20 [\n",
            "       [ [0. 0. 0. 0.], [-1.76953125 -2.11523438 -0.875      -2.51162871], [-2.5838028  -2.37335938 -1.77851367 -2.76416579], [-3.03573402 -2.26230957 -2.54756506 -2.93275184], ]\n",
            "       [ [-0.9375     -2.31046875 -1.503125   -2.13773437], [-1.546875   -2.13734375 -1.78081055 -2.39704102], [-2.48645879 -1.88625    -2.1640625  -2.15803711], [-2.97792896 -1.65234375 -2.64673096 -2.21639771], ]\n",
            "       [ [-1.6078125  -2.66015039 -2.37128557 -2.49339404], [-2.09507813 -1.86705469 -2.41644043 -1.86125   ], [-2.3483584 -1.4859375 -2.2393125 -1.7015625], [-1.5078125  -0.99609375 -1.2        -1.63828125], ]\n",
            "       [ [-2.40639801 -2.47228906 -2.90832367 -2.4630293 ], [-1.889375   -2.30660937 -2.98715615 -1.8021875 ], [-2.07936094 -1.4234375  -2.2265     -0.96875   ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, right, ]\n",
            "       [ up, right&left&down&up, left, down, ]\n",
            "       [ up, right, right, right, ]\n",
            "       [ up, right, up, stay, ]\n",
            "]\n",
            "episode #24 [\n",
            "       [ [0. 0. 0. 0.], [-1.76953125 -2.11523438 -0.9375     -2.51162871], [-2.5838028  -2.37335938 -1.77851367 -2.95070216], [-3.18041104 -2.60320496 -2.57411368 -3.27386835], ]\n",
            "       [ [-0.9375     -2.31046875 -1.503125   -2.13773437], [-1.6671875  -2.13734375 -1.78081055 -2.39704102], [-2.48645879 -2.11179688 -2.1640625  -2.61420349], [-3.17322849 -1.8659668  -2.69444412 -2.66490223], ]\n",
            "       [ [-1.6078125  -2.66015039 -2.37128557 -2.49339404], [-2.24363281 -1.86705469 -2.41644043 -2.09929688], [-2.3483584  -1.4859375  -2.2393125  -1.84907227], [-2.51575928 -0.99902344 -1.2        -1.89006348], ]\n",
            "       [ [-2.40639801 -2.47228906 -2.90832367 -2.54249902], [-2.4786875  -2.46428906 -2.98715615 -1.8021875 ], [-2.07936094 -1.4234375  -2.2265     -0.984375  ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, right, ]\n",
            "       [ up, right&left&down&up, left, down, ]\n",
            "       [ up, right, right, right, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #28 [\n",
            "       [ [0. 0. 0. 0.], [-1.80664062 -2.11523438 -0.96875    -2.57082368], [-2.62978924 -2.86766357 -1.81113184 -3.30577418], [-3.33461963 -2.68206951 -2.6120219  -3.33267227], ]\n",
            "       [ [-0.96875    -2.31046875 -1.503125   -2.13773437], [-1.6671875  -2.13734375 -1.78081055 -2.39704102], [-2.61177162 -2.22457031 -2.1640625  -2.61420349], [-3.29876808 -1.88254395 -2.69444412 -2.67213617], ]\n",
            "       [ [-1.6078125  -2.92209678 -2.37128557 -2.58687163], [-2.24363281 -1.86705469 -2.41644043 -2.21832031], [-2.76255249 -1.4859375  -2.2393125  -1.87409668], [-2.51575928 -0.99951172 -1.76867187 -1.89006348], ]\n",
            "       [ [-2.43687294 -2.47228906 -2.90832367 -2.54249902], [-2.4786875  -2.46428906 -2.98715615 -1.8440625 ], [-2.07936094 -1.4234375  -2.42423438 -0.9921875 ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, right, ]\n",
            "       [ up, right&left&down&up, left, down, ]\n",
            "       [ up, right, right, right, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #32 [\n",
            "       [ [0. 0. 0. 0.], [-1.80664062 -2.11523438 -0.9921875  -2.57082368], [-2.62978924 -2.86766357 -1.84853467 -3.30577418], [-3.33461963 -2.68206951 -2.62102028 -3.33267227], ]\n",
            "       [ [-0.96875    -2.31046875 -1.503125   -2.31910156], [-1.76953125 -2.56691602 -1.78081055 -2.39704102], [-2.61177162 -2.22457031 -2.1640625  -2.68001114], [-3.29876808 -1.89541626 -2.82105019 -2.6870416 ], ]\n",
            "       [ [-1.6078125  -2.92209678 -2.37128557 -2.58687163], [-2.41810547 -2.46150586 -2.41644043 -2.27783203], [-2.85510437 -1.4859375  -2.45983086 -1.88693848], [-2.60502441 -0.99987793 -2.05300781 -1.89492188], ]\n",
            "       [ [-2.43687294 -2.47228906 -2.90832367 -2.54249902], [-2.86703809 -2.65660364 -2.98715615 -1.8440625 ], [-2.07936094 -1.4234375  -2.42423438 -0.9921875 ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, right&left&down&up, left, down, ]\n",
            "       [ up, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #36 [\n",
            "       [ [0. 0. 0. 0.], [-1.89469604 -2.35390625 -0.99902344 -2.63553938], [-2.69943185 -2.94703131 -1.89553526 -3.40415088], [-3.412759   -2.70871129 -2.69375692 -3.39954248], ]\n",
            "       [ [-0.984375   -2.37875    -1.503125   -2.31910156], [-1.76953125 -2.56691602 -1.78081055 -2.87882935], [-2.69827415 -2.30915039 -2.1640625  -2.69294289], [-3.41148766 -1.8993309  -2.82105019 -2.69645812], ]\n",
            "       [ [-1.6078125  -2.92209678 -2.37128557 -2.93425453], [-2.54895996 -2.46150586 -2.41644043 -2.27783203], [-2.92451828 -1.68945312 -2.45983086 -1.88693848], [-2.69603657 -0.99987793 -2.05300781 -1.89492188], ]\n",
            "       [ [-2.43687294 -2.47228906 -2.90832367 -2.54249902], [-2.86703809 -2.65660364 -2.98715615 -1.8440625 ], [-2.29993438 -1.4234375  -2.42423438 -0.9921875 ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, right&left&down&up, left, down, ]\n",
            "       [ up, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #40 [\n",
            "       [ [0. 0. 0. 0.], [-1.89469604 -2.35390625 -0.99975586 -2.63553938], [-2.69943185 -2.94703131 -1.89732818 -3.40415088], [-3.412759   -2.70871129 -2.69375692 -3.39954248], ]\n",
            "       [ [-0.984375   -2.37875    -1.503125   -2.31910156], [-1.8345459  -2.56691602 -1.78081055 -2.87882935], [-2.69827415 -2.30915039 -2.1640625  -2.69294289], [-3.41148766 -1.8993309  -2.82105019 -2.69645812], ]\n",
            "       [ [-1.6078125  -2.92209678 -2.37128557 -2.93425453], [-2.54895996 -2.46150586 -2.41644043 -2.27783203], [-2.92451828 -1.68945312 -2.45983086 -1.88693848], [-2.69603657 -0.99993896 -2.05300781 -1.89492188], ]\n",
            "       [ [-2.43687294 -2.47228906 -2.90832367 -2.54249902], [-2.86703809 -2.65660364 -2.98715615 -1.8440625 ], [-2.29993438 -1.4234375  -2.42423438 -0.99609375], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, left, left, down, ]\n",
            "       [ up, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw7IhLpiP2J"
      },
      "source": [
        "## Tester cells (to be ignored)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2coXnct5GI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eba8d97-c08b-4a1d-b3a7-86f1546dbf10"
      },
      "source": [
        "a = deque(maxlen=3)\n",
        "a.append(1)\n",
        "a.append(2)\n",
        "a.append(3)\n",
        "a[2]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIiMNeIid7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
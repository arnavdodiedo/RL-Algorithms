{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ADkFcuITboaQ",
        "3sw7IhLpiP2J"
      ],
      "authorship_tag": "ABX9TyN0o13hfIxbRLxIwOExdkrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPTWDQ1btQo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIncFsMcHDI"
      },
      "source": [
        "## Policy evalution with different solutions\n",
        "#### where policy is a uniform one, equal probabilites of going to each neighbouring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCe0vwSXOv6"
      },
      "source": [
        "class gridworld():\n",
        "    def __init__(self, size = 4, reward = 0, penalty_per_step = -1, epochs = 100, gamma = 0.9, epsilon = 0.1, epsilon_decay_factor = 0.1):\n",
        "        self.size = size # square grid world size\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.returngrid = np.zeros((size, size), dtype=np.float) # reward for all states initialised to 0        \n",
        "        self.goal_states = [[0,0],[size-1, size-1]] # goal states are the top left and bottom right corners of the grid\n",
        "        self.reward = reward # 0 reward on reaching goal state\n",
        "        self.epsilon = epsilon\n",
        "        # terminate on reaching goal state OR stuck in infinite loop with 0 rewards per step in this loop\n",
        "\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # state action value matrix\n",
        "\n",
        "        self.penalty_per_step = penalty_per_step # -1 reward per movement \n",
        "        self.movements = [[-1,0], [1,0], [0,-1], [0,1]]\n",
        "        self.episodes = self.epochs = epochs # number of epochs to run policy evalution\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def reset_grid(self):\n",
        "        self.returngrid = np.zeros((self.size, self.size))\n",
        "        self.epsilon = 0.1\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # reset state action value matrix\n",
        "\n",
        "    def greedy_policy_update_dp(self, curr_state):  # update policy greedily, second step of generalized policy improvement (GPI)\n",
        "        if curr_state not in self.goal_states:\n",
        "            self.policy[curr_state[0]][curr_state[1]] = np.zeros(4)\n",
        "            \n",
        "            up = self.next_state(curr_state, 0)\n",
        "            up = self.returngrid[up[0]][up[1]]\n",
        "\n",
        "            down = self.next_state(curr_state, 1)\n",
        "            down = self.returngrid[down[0]][down[1]]\n",
        "            \n",
        "            left = self.next_state(curr_state, 2)\n",
        "            left = self.returngrid[left[0]][left[1]]\n",
        "            \n",
        "            right = self.next_state(curr_state, 3)\n",
        "            right = self.returngrid[right[0]][right[1]]\n",
        "\n",
        "            arr = [[up, 0], [down, 1], [left, 2], [right, 3]]            \n",
        "            arr.sort()       \n",
        "            arr.reverse()     \n",
        "\n",
        "            m = [arr[0][1]]\n",
        "            for i in range(1, 4):\n",
        "                if arr[i][0] != arr[i-1][0]: break\n",
        "                else: m.append(arr[i][1])\n",
        "            \n",
        "            value = 1/len(m)\n",
        "            \n",
        "            for i in m:\n",
        "                self.policy[curr_state[0]][curr_state[1]][i] = value\n",
        "            \n",
        "            # print(self.policy[curr_state[0]][curr_state[1]])\n",
        "\n",
        "\n",
        "    def next_state(self, curr_state, direction): # direction = 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        change = self.movements[direction]\n",
        "        new_state = [curr_state[0]+change[0], curr_state[1]+change[1]]\n",
        "\n",
        "        if(new_state[0]<0): new_state[0] = 0\n",
        "        elif(new_state[0]>self.size-1): new_state[0] = self.size-1\n",
        "        \n",
        "        if(new_state[1]<0): new_state[1] = 0\n",
        "        elif(new_state[1]>self.size-1): new_state[1] = self.size-1\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    def display_grid(self):                \n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.returngrid[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_state_action_values(self):\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.state_action_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_policy(self):\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):                \n",
        "                \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"stay\", end=\", \")\n",
        "                    continue\n",
        "\n",
        "                probs = [[self.policy[i][j][0], 0], [self.policy[i][j][1], 1], [self.policy[i][j][2], 2], [self.policy[i][j][3], 3]]\n",
        "                probs.sort()\n",
        "                probs.reverse()\n",
        "\n",
        "                moves = [\"up\", \"down\", \"left\", \"right\"]\n",
        "                m = [probs[0][1]]\n",
        "\n",
        "                for k in range(1,4):\n",
        "                    if probs[k][0] != probs[k-1][0]: break\n",
        "                    else: m.append(probs[k][1])\n",
        "                \n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end=\"&\")\n",
        "                    else: print(moves[m[k]], end=\", \")\n",
        "                \n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def evaluate_current_policy_dp(self):\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):                        \n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.returngrid[new_state[0], new_state[1]]) * self.policy[i][j][k]                                \n",
        "                    \n",
        "                    self.returngrid[i][j] = value\n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                \n",
        "    \n",
        "    def gpi_dp(self):\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):\n",
        "            # either update in self.returngrid as you traverse OR in each epoch maintain a copy of self.returngrid and use it for update at the end\n",
        "            returngrid_copy = np.copy(self.returngrid)\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.returngrid[new_state[0]][new_state[1]]) * self.policy[i][j][k]                    \n",
        "                    returngrid_copy[i,j] = value\n",
        "\n",
        "            if (self.returngrid == returngrid_copy).all():                 \n",
        "                print(\"Policy did not improve. Stopping...\")\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "                break\n",
        "            else: \n",
        "                self.returngrid = returngrid_copy\n",
        "            \n",
        "            for i in range(self.size): \n",
        "                for j in range(self.size): \n",
        "                    self.greedy_policy_update_dp([i,j])            \n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "\n",
        "    def evaluate_current_policy_monte_carlo(self): # first visit monte carlo\n",
        "        self.reset_grid() # start from scratch\n",
        "\n",
        "        number_of_visits = np.zeros((self.size, self.size))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            k = np.random.choice(range(16))\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "\n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []\n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]] == 0:\n",
        "                    number_of_visits[state[0],state[1]] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]] = 1\n",
        "                \n",
        "                path.append(state)\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                direction = np.random.choice(range(4), p=self.policy[state[0]][state[1]])\n",
        "                new_state = self.next_state(state, direction)            \n",
        "                state = new_state\n",
        "                \n",
        "            state_returns = np.zeros_like(state_reward, dtype=np.float)            \n",
        "            # print(state_returns.shape, len(state_reward))\n",
        "            state_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_returns[i] = state_reward[i] + self.gamma * state_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            for p in range(len(path)):\n",
        "                if is_visited_this_episode[path[p][0]][path[p][1]] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[path[p][0]][path[p][1]] = 1\n",
        "                \n",
        "                self.returngrid[path[p][0]][path[p][1]] += (state_returns[p]-self.returngrid[path[p][0]][path[p][1]])/number_of_visits[path[p][0]][path[p][1]]\n",
        "            \n",
        "            if (_%1000 == 0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()\n",
        "    \n",
        "    def epsilon_greedy_policy_update_monte_carlo(self, curr_state): # epsilon greedy update of policy, second step of monte carlo policy improvement\n",
        "        i, j = curr_state[0], curr_state[1]\n",
        "        chance = np.random.uniform(0,1)\n",
        "        \n",
        "        if chance <= self.epsilon:\n",
        "            self.policy[i][j] = np.zeros(4) + 0.25\n",
        "        else:            \n",
        "            q_vals = [[self.state_action_values[i][j][0], 0], [self.state_action_values[i][j][1], 1], [self.state_action_values[i][j][2], 2], [self.state_action_values[i][j][3], 3]]\n",
        "            q_vals.sort()\n",
        "            q_vals.reverse()\n",
        "            \n",
        "            arr = [q_vals[0][1]]\n",
        "\n",
        "            for k in range(1, 4):\n",
        "                if q_vals[k-1][0] != q_vals[k][0]: break\n",
        "                else:\n",
        "                    arr.append(q_vals[k][1])\n",
        "            \n",
        "            value = 1/len(arr)\n",
        "\n",
        "            if len(arr)==1 and self.next_state(curr_state, arr[0]) == curr_state:\n",
        "                prob = np.zeros(4) + 0.25\n",
        "            else:\n",
        "                prob = np.zeros(4)        \n",
        "                for k in arr:\n",
        "                    prob[k] = value\n",
        "            \n",
        "            self.policy[i][j] = prob\n",
        "        \n",
        "        # print(\"updated policy at\", curr_state, \"to\", self.policy[i][j])\n",
        "\n",
        "    def gpi_monte_carlo(self):\n",
        "        self.reset_grid()\n",
        "        number_of_visits = np.zeros((self.size, self.size, 4))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # print(\"epoch\", _)\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "\n",
        "            k = np.random.choice(range(16))\n",
        "            action = np.random.choice([0,1,2,3])\n",
        "\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "\n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []            \n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]][action] == 0:\n",
        "                    number_of_visits[state[0]][state[1]][action] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]][action] = 1\n",
        "\n",
        "                # print(\"in state\", state, \"took action\", action, \"with policy\", self.policy[state[0]][state[1]])  \n",
        "                if path.count([state, action, self.policy[state[0]][state[1]].tolist()]) > 3: break\n",
        "\n",
        "                path.append([state, action, self.policy[state[0]][state[1]].tolist()])\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                action = np.random.choice(range(4), p=self.policy[state[0]][state[1]])                \n",
        "                state = self.next_state(state, action)                                \n",
        "                \n",
        "            state_action_returns = np.zeros_like(state_reward, dtype=np.float)            \n",
        "            # print(state_returns.shape, len(state_reward))\n",
        "            state_action_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_action_returns[i] = state_reward[i] + self.gamma * state_action_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1                \n",
        "                self.state_action_values[x][y][a] += (state_action_returns[p]-self.state_action_values[x][y][a])/number_of_visits[x][y][a]\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1                                \n",
        "                self.epsilon_greedy_policy_update_monte_carlo([x,y])\n",
        "            \n",
        "            if _%100: self.epsilon *= self.epsilon_decay_factor\n",
        "\n",
        "            if (_%(self.epochs/10) == 0):                \n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_state_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkFcuITboaQ"
      },
      "source": [
        "## DP solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gVHaw87EBs",
        "outputId": "3c02cf57-abfa-46eb-ca09-4f81c4186184"
      },
      "source": [
        "grid = gridworld(epochs=100)\n",
        "grid.gpi_dp()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy did not improve. Stopping...\n",
            "epoch #4 [\n",
            "       [ 0.0, -1.0, -2.0, -3.0, ]\n",
            "       [ -1.0, -2.0, -3.0, -2.0, ]\n",
            "       [ -2.0, -3.0, -2.0, -1.0, ]\n",
            "       [ -3.0, -2.0, -1.0, 0.0, ]\n",
            "]\n",
            "policy -\n",
            "[\n",
            "       [ stay, left, left, left&down, ]\n",
            "       [ up, left&up, right&left&down&up, down, ]\n",
            "       [ up, right&left&down&up, right&down, down, ]\n",
            "       [ right&up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmWnDXbz-Q"
      },
      "source": [
        "## Monte Carlo solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI3VPo26ZBP4",
        "outputId": "1a2e2fa9-9543-427d-9f5d-44297845acbb"
      },
      "source": [
        "grid = gridworld(size = 4, epochs=10000, gamma = 0.9, epsilon_decay_factor = 0.5)\n",
        "grid.gpi_monte_carlo()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #1000 [\n",
            "       [ [0. 0. 0. 0.], [-1.08867666 -1.34457597 -1.         -2.12588835], [-4.00954926 -4.8998378  -4.85834375 -3.7972893 ], [-5.12380581 -3.52517326 -4.9234849  -3.82246268], ]\n",
            "       [ [-5.09670771 -5.13484798 -5.08211795 -4.74790233], [-2.67515035 -4.2188532  -5.2654033  -4.65032766], [-4.63809489 -5.89623428 -6.33854873 -5.32578918], [-5.59851795 -2.62816732 -2.88577891 -3.39014828], ]\n",
            "       [ [-5.6675695  -6.05940803 -5.75885228 -6.06520402], [-4.07889191 -4.69615606 -5.69638959 -4.7085379 ], [-4.62571347 -5.88683874 -5.21386856 -4.71545971], [-2.30856936 -1.39999062 -2.63749504 -2.30524673], ]\n",
            "       [ [-6.02581104 -6.01976381 -5.9799783  -6.29521077], [-4.77970713 -6.18227096 -5.62989818 -5.25373705], [-4.90980694 -4.13087929 -4.98083227 -5.52090472], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, down, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ right&left&down&up, up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #2000 [\n",
            "       [ [0. 0. 0. 0.], [-1.02803595 -1.1413645  -1.         -1.44442961], [-3.64325058 -4.35524067 -4.53025374 -3.6728389 ], [-4.62603643 -3.19910396 -4.00756011 -3.21110157], ]\n",
            "       [ [-2.93426578 -3.74182953 -4.58525949 -3.08619211], [-2.19269163 -3.11739793 -4.04812977 -3.61895479], [-4.26888231 -4.9264045  -5.65177871 -5.06820307], [-4.41813988 -2.20273915 -2.39288946 -2.73593684], ]\n",
            "       [ [-4.05502815 -5.00841645 -4.324375   -4.64873101], [-3.42946412 -3.86859103 -5.50162505 -3.70926895], [-4.68103132 -5.63394427 -5.1624051  -4.71222541], [-1.67851744 -1.15737336 -1.77565555 -2.06211253], ]\n",
            "       [ [-4.7103137  -5.59415439 -5.5915561  -4.97731044], [-4.23349312 -5.09531454 -4.96480409 -5.04179696], [-4.78673039 -3.67436077 -4.25107556 -4.97608409], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right&left&down&up, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #3000 [\n",
            "       [ [0. 0. 0. 0.], [-1.01554721 -1.10810227 -1.         -1.35932607], [-3.60106127 -4.16047205 -4.43084311 -3.5869597 ], [-4.32429364 -3.04731307 -3.65073108 -3.06329572], ]\n",
            "       [ [-2.58047194 -2.99593919 -4.25228605 -2.58208679], [-2.0405153  -2.76956995 -3.58270165 -3.38934711], [-4.21745468 -4.41513257 -5.27081105 -4.81284163], [-4.09171434 -2.09188875 -2.20001967 -2.53469279], ]\n",
            "       [ [-3.81005677 -4.62884778 -3.78727985 -3.77222257], [-3.13351404 -3.63687283 -5.10282147 -3.14200161], [-4.67469227 -5.447185   -4.9000215  -4.7077524 ], [-1.46974285 -1.09999766 -1.58949822 -1.86668383], ]\n",
            "       [ [-4.51006266 -5.36017325 -5.39085183 -4.48000088], [-3.85152527 -4.7895334  -4.59366256 -4.4869267 ], [-4.43767698 -3.74116776 -4.32543731 -4.73231882], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right, up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #4000 [\n",
            "       [ [0. 0. 0. 0.], [-1.01026702 -1.08353357 -1.         -1.28147209], [-3.55147959 -4.01373197 -4.28253012 -3.54996977], [-4.12081125 -2.96082254 -3.35878006 -2.96305419], ]\n",
            "       [ [-2.59334429 -2.86199325 -4.1206363  -2.57712032], [-1.99318188 -2.62680473 -3.28304245 -3.00602989], [-4.17750213 -4.37322354 -5.03566884 -4.63778269], [-3.59075106 -2.03590549 -2.14212114 -2.35697881], ]\n",
            "       [ [-3.80091999 -4.50971584 -3.76839152 -3.87039412], [-3.01921724 -3.45942895 -4.89888782 -3.0352536 ], [-4.6782998  -5.32368311 -4.87253028 -4.70541366], [-1.33925872 -1.0709343  -1.41513959 -1.76833673], ]\n",
            "       [ [-4.42832759 -5.14761998 -5.20122961 -4.3911776 ], [-3.78498894 -4.60946228 -4.3525558  -4.16474125], [-4.24486882 -3.89660854 -4.03355606 -4.56668923], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, down, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right&left&down&up, up, up, down, ]\n",
            "       [ right, up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #5000 [\n",
            "       [ [0. 0. 0. 0.], [-1.00758641 -1.06806439 -1.         -1.2193289 ], [-3.61981811 -3.97386067 -4.23091138 -3.57301003], [-4.01722417 -3.0963408  -3.31854498 -3.00998584], ]\n",
            "       [ [-2.58492074 -2.8114882  -3.97480534 -2.56944317], [-1.96789094 -2.47289314 -3.0604839  -2.80471305], [-4.24477078 -4.35638586 -4.90907603 -4.64746724], [-3.4173407  -2.01184462 -2.09715578 -2.27663088], ]\n",
            "       [ [-3.82518697 -4.46271332 -3.81516497 -3.86327923], [-2.93692227 -3.2697001  -4.70608423 -2.97147838], [-4.69643191 -5.28523134 -4.93666469 -4.71066897], [-1.28624955 -1.05802416 -1.33879208 -1.64103833], ]\n",
            "       [ [-4.38448185 -4.93413715 -5.10853719 -4.30893382], [-3.7175885  -4.42534462 -4.17909584 -3.984605  ], [-4.17003216 -3.86066001 -3.91315024 -4.53241363], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, right&left&down&up, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right&left&down&up, up, up, down, ]\n",
            "       [ right, up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #6000 [\n",
            "       [ [0. 0. 0. 0.], [-1.00589431 -1.05803385 -1.         -1.17777184], [-3.79315633 -4.08428103 -4.17226463 -3.64130176], [-3.92170372 -3.36000992 -3.48208012 -3.2210021 ], ]\n",
            "       [ [-2.60143487 -2.79017568 -3.90626909 -2.58738659], [-1.95331722 -2.3727759  -2.8898245  -2.61998106], [-4.40829789 -4.46614564 -4.91870184 -4.70510712], [-3.14581657 -1.99515254 -2.06830372 -2.22641343], ]\n",
            "       [ [-3.79885877 -4.38460715 -3.80012043 -3.7839747 ], [-2.88756083 -3.19061205 -4.56213585 -2.92981424], [-4.73547233 -5.30605809 -5.02181407 -4.79255335], [-1.23189837 -1.04897844 -1.28896971 -1.57320359], ]\n",
            "       [ [-4.32906831 -4.86180636 -5.01593887 -4.26244821], [-3.66245119 -4.21585549 -4.01221148 -3.88901672], [-4.09293609 -3.87413608 -3.84733905 -4.50673582], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, right&left&down&up, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right, up, left, stay, ]\n",
            "]\n",
            "epoch #7000 [\n",
            "       [ [0. 0. 0. 0.], [-1.00482716 -1.05012014 -1.         -1.15783482], [-3.95459398 -4.15655212 -4.2856358  -3.75944866], [-3.97093575 -3.47333843 -3.65518911 -3.40539961], ]\n",
            "       [ [-2.60697648 -2.78158543 -3.81383012 -2.59917634], [-1.94288066 -2.31979239 -2.77036292 -2.53566895], [-4.53371803 -4.54711496 -5.0195804  -4.76851476], [-3.02716738 -1.98279581 -2.03800905 -2.18092959], ]\n",
            "       [ [-3.78014611 -4.27771243 -3.77568371 -3.74918734], [-2.8552226  -3.09786235 -4.46111026 -2.8726268 ], [-4.78548723 -5.19949598 -5.0695518  -4.79651781], [-1.19698894 -1.04235195 -1.23770089 -1.49924184], ]\n",
            "       [ [-4.2897285  -4.81524525 -4.9122408  -4.24034561], [-3.64726519 -4.09903512 -3.89697053 -3.82128845], [-4.09313703 -3.89623247 -3.88598067 -4.46611386], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, right&left&down&up, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right, up, left, stay, ]\n",
            "]\n",
            "epoch #8000 [\n",
            "       [ [0. 0. 0. 0.], [-1.00416975 -1.04446142 -1.         -1.13403433], [-4.03590317 -4.26276838 -4.31296917 -3.88898762], [-3.97615204 -3.58319412 -3.76937152 -3.50786563], ]\n",
            "       [ [-2.60567164 -2.77313045 -3.71597742 -2.60977687], [-1.93675804 -2.27748773 -2.66486439 -2.47618596], [-4.56777722 -4.62387681 -5.04803055 -4.87810373], [-2.88627145 -1.97527301 -2.01220248 -2.14835805], ]\n",
            "       [ [-3.75605029 -4.1925307  -3.73359824 -3.71041392], [-2.83337679 -3.04497203 -4.35634298 -2.82906605], [-4.74083118 -5.0343706  -4.7807453  -4.03001217], [-1.17121468 -1.03801055 -1.22329478 -1.30952994], ]\n",
            "       [ [-4.27390505 -4.77923799 -4.85095524 -4.22233996], [-3.61926314 -3.99110485 -3.81702309 -3.78071673], [-4.09330382 -3.91634267 -3.90826388 -4.42879294], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, right&left&down&up, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ right, right, right, down, ]\n",
            "       [ right, up, left, stay, ]\n",
            "]\n",
            "epoch #10000 [\n",
            "       [ [0. 0. 0. 0.], [-1.00385055 -1.03603409 -1.         -1.10424892], [-4.12227288 -4.35952215 -4.3235078  -4.04942789], [-3.93109623 -3.6964832  -3.9210268  -3.61414785], ]\n",
            "       [ [-2.59828452 -2.73909853 -3.60454856 -2.59352685], [-1.93624804 -2.20059208 -2.51562256 -2.39407084], [-4.59667419 -4.7257612  -5.0570679  -4.97278451], [-2.70511955 -1.9629444  -1.9890381  -2.09926401], ]\n",
            "       [ [-3.72234394 -4.02355715 -3.68777185 -3.65613114], [-2.80432311 -2.97009593 -4.18603164 -2.79001239], [-4.30784204 -4.66920121 -4.25697343 -2.60654062], [-1.13774414 -1.03141745 -1.17863582 -1.09363481], ]\n",
            "       [ [-4.23085939 -4.69806907 -4.75475548 -4.19379253], [-3.57563257 -3.90102458 -3.72328317 -3.71147735], [-4.07046261 -3.90395375 -3.88267214 -4.30374214], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, right&left&down&up, ]\n",
            "       [ right, up, up, down, ]\n",
            "       [ right, right, right, down, ]\n",
            "       [ right, up, left, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw7IhLpiP2J"
      },
      "source": [
        "## Tester module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2coXnct5GI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIiMNeIid7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
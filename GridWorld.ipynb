{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ADkFcuITboaQ"
      ],
      "authorship_tag": "ABX9TyP1VJwynqD1ZSizPr2mkKAi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPTWDQ1btQo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIncFsMcHDI"
      },
      "source": [
        "## Generalized policy iteration with DP, Monte Carlo, TD learning and Q learning\n",
        "#### where the initial policy is a uniform one, equal probabilites of going to each neighbouring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCe0vwSXOv6"
      },
      "source": [
        "class gridworld():\n",
        "    def __init__(self, size = 4, reward = 0, penalty_per_step = -1, epochs = 100, gamma = 0.99, epsilon_decay_factor = 0.99, seed = 0):\n",
        "        self.size = size # square grid world size\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.returngrid = np.zeros((size, size), dtype=np.float) # reward for all states initialised to 0        \n",
        "        self.goal_states = [[0,0],[size-1, size-1]] # goal states are the top left and bottom right corners of the grid\n",
        "        self.reward = reward # 0 reward on reaching goal state\n",
        "        self.epsilon = 1\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # terminate on reaching goal state OR stuck in infinite loop with 0 rewards per step in this loop\n",
        "\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # state action value matrix\n",
        "\n",
        "        self.penalty_per_step = penalty_per_step # -1 reward per movement \n",
        "        self.movements = [[-1,0], [1,0], [0,-1], [0,1]]\n",
        "        self.episodes = self.epochs = epochs # number of epochs to run policy evalution\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def reset_grid(self): # reset all agent and environment variables\n",
        "        self.returngrid = np.zeros((self.size, self.size))\n",
        "        self.epsilon = 1\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # reset state action value matrix\n",
        "\n",
        "    def greedy_policy_update_dp(self, curr_state):  # update policy greedily, second step of generalized policy iteration (GPI)\n",
        "        if curr_state not in self.goal_states:\n",
        "            self.policy[curr_state[0]][curr_state[1]] = np.zeros(4)\n",
        "            \n",
        "            up = self.next_state(curr_state, 0)\n",
        "            up = self.returngrid[up[0]][up[1]]\n",
        "\n",
        "            down = self.next_state(curr_state, 1)\n",
        "            down = self.returngrid[down[0]][down[1]]\n",
        "            \n",
        "            left = self.next_state(curr_state, 2)\n",
        "            left = self.returngrid[left[0]][left[1]]\n",
        "            \n",
        "            right = self.next_state(curr_state, 3)\n",
        "            right = self.returngrid[right[0]][right[1]]\n",
        "\n",
        "            arr = [[up, 0], [down, 1], [left, 2], [right, 3]]            \n",
        "            arr.sort()       \n",
        "            arr.reverse()     \n",
        "\n",
        "            m = [arr[0][1]]\n",
        "            for i in range(1, 4):\n",
        "                if arr[i][0] != arr[i-1][0]: break\n",
        "                else: m.append(arr[i][1])\n",
        "            \n",
        "            value = 1/len(m)\n",
        "            \n",
        "            for i in m:\n",
        "                self.policy[curr_state[0]][curr_state[1]][i] = value\n",
        "            \n",
        "            # print(self.policy[curr_state[0]][curr_state[1]])\n",
        "\n",
        "\n",
        "    def next_state(self, curr_state, direction): # direction = 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        change = self.movements[direction]\n",
        "        new_state = [curr_state[0]+change[0], curr_state[1]+change[1]]        \n",
        "\n",
        "        if(new_state[0]<0): new_state[0] = 0\n",
        "        elif(new_state[0]>self.size-1): new_state[0] = self.size-1\n",
        "        \n",
        "        if(new_state[1]<0): new_state[1] = 0\n",
        "        elif(new_state[1]>self.size-1): new_state[1] = self.size-1\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    def display_grid(self): # display the state of the grid world                \n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.returngrid[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_state_action_values(self): # display state action values\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.state_action_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_policy(self): # display policy at each state, for each of the 4 actions\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):                \n",
        "                \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"stay\", end=\", \")\n",
        "                    continue\n",
        "\n",
        "                probs = [[self.policy[i][j][0], 0], [self.policy[i][j][1], 1], [self.policy[i][j][2], 2], [self.policy[i][j][3], 3]]\n",
        "                probs.sort()\n",
        "                probs.reverse()\n",
        "\n",
        "                moves = [\"up\", \"down\", \"left\", \"right\"]\n",
        "                m = [probs[0][1]]\n",
        "\n",
        "                for k in range(1,4):\n",
        "                    if probs[k][0] != probs[k-1][0]: break\n",
        "                    else: m.append(probs[k][1])\n",
        "                \n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end=\"&\")\n",
        "                    else: print(moves[m[k]], end=\", \")\n",
        "                \n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def evaluate_current_policy_dp(self): # policy evaluation using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):                        \n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.returngrid[new_state[0], new_state[1]]) * self.policy[i][j][k]                                \n",
        "                    \n",
        "                    self.returngrid[i][j] = value\n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                \n",
        "    \n",
        "    def gpi_dp(self): # GPI using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):\n",
        "            # either update in self.returngrid as you traverse OR in each epoch maintain a copy of self.returngrid and use it for update at the end\n",
        "            returngrid_copy = np.copy(self.returngrid)\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.returngrid[new_state[0]][new_state[1]]) * self.policy[i][j][k]                    \n",
        "                    returngrid_copy[i,j] = value\n",
        "\n",
        "            if (self.returngrid == returngrid_copy).all():                 \n",
        "                print(\"Policy did not improve. Stopping...\")\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "                break\n",
        "            else: \n",
        "                self.returngrid = returngrid_copy\n",
        "            \n",
        "            for i in range(self.size): \n",
        "                for j in range(self.size): \n",
        "                    self.greedy_policy_update_dp([i,j])            \n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "\n",
        "    def evaluate_current_policy_monte_carlo(self): # evaluate policy using first visit monte carlo\n",
        "        self.reset_grid() # start from scratch\n",
        "\n",
        "        number_of_visits = np.zeros((self.size, self.size))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []\n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]] == 0:\n",
        "                    number_of_visits[state[0],state[1]] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]] = 1\n",
        "                \n",
        "                path.append(state)\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                direction = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                new_state = self.next_state(state, direction)            \n",
        "                state = new_state\n",
        "                \n",
        "            state_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_returns[i] = state_reward[i] + self.gamma * state_returns[i+1]\n",
        "            \n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            for p in range(len(path)):\n",
        "                if is_visited_this_episode[path[p][0]][path[p][1]] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[path[p][0]][path[p][1]] = 1\n",
        "                \n",
        "                self.returngrid[path[p][0]][path[p][1]] += (state_returns[p]-self.returngrid[path[p][0]][path[p][1]])/number_of_visits[path[p][0]][path[p][1]]\n",
        "            \n",
        "            if (_%1000 == 0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()\n",
        "    \n",
        "    def epsilon_greedy_policy_update_monte_carlo(self): # epsilon greedy update of policy, second step of monte carlo policy improvement\n",
        "        \n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "\n",
        "                if [i,j] in self.goal_states: continue\n",
        "\n",
        "                prob = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)        \n",
        "                prob[np.argmax(self.state_action_values[i][j])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "\n",
        "                self.policy[i][j] = prob\n",
        "            \n",
        "                # print(\"updated policy at\", curr_state, \"to\", self.policy[i][j])\n",
        "\n",
        "    def gpi_monte_carlo(self): # GPI using epsilon greedy first visit monte carlo\n",
        "        self.reset_grid()\n",
        "        number_of_visits = np.zeros((self.size, self.size, 4))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # print(\"epoch\", _)\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            \n",
        "            k = np.random.choice(range(1, self.size*self.size-1))            \n",
        "            i = k//4\n",
        "            j = k - 4*i                                    \n",
        "            \n",
        "            # action = np.argmax(self.policy[i][j])\n",
        "            action = np.random.choice([0,1,2,3])\n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []            \n",
        "\n",
        "            while True:\n",
        "                if is_visited_this_episode[state[0]][state[1]][action] == 0:\n",
        "                    number_of_visits[state[0]][state[1]][action] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]][action] = 1\n",
        "\n",
        "                # print(\"in state\", state, \"took action\", action, \"with policy\", self.policy[state[0]][state[1]])  \n",
        "                # if path.count([state, action]) > 3: break                \n",
        "\n",
        "                path.append([state, action])\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "\n",
        "                # IMPORTANT STEP - first select next state based on current state and action, \n",
        "                # then select next action based on probabilities in this new state\n",
        "                state = self.next_state(state, action)                                \n",
        "\n",
        "                if state in self.goal_states: break\n",
        "\n",
        "                pr = self.policy[state[0]][state[1]]\n",
        "                # pr[np.argmax(pr)] -= .1\n",
        "                # pr[np.random.choice(np.delete(np.arange(4), np.argmax(pr)))] += .05\n",
        "                # pr[np.random.choice(np.delete(np.arange(4), np.argmax(pr)))] += .05\n",
        "                \n",
        "                action = np.random.choice(range(len(self.movements)), p=pr)                                \n",
        "                \n",
        "            state_action_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_action_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_action_returns[i] = state_reward[i] + self.gamma * state_action_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1   \n",
        "\n",
        "                # self.state_action_values[x][y][a] += (state_action_returns[p]-self.state_action_values[x][y][a])/number_of_visits[x][y][a]\n",
        "                self.state_action_values[x][y][a] = np.mean(state_action_returns[p:])\n",
        "                        \n",
        "            self.epsilon_greedy_policy_update_monte_carlo() # improve the policy based on updated state-action values                                                        \n",
        "\n",
        "            if (_%(self.epochs/10) == 0):                \n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_state_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor          "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkFcuITboaQ"
      },
      "source": [
        "## Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gVHaw87EBs",
        "outputId": "3c02cf57-abfa-46eb-ca09-4f81c4186184"
      },
      "source": [
        "grid = gridworld(epochs=100)\n",
        "grid.gpi_dp()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy did not improve. Stopping...\n",
            "epoch #4 [\n",
            "       [ 0.0, -1.0, -2.0, -3.0, ]\n",
            "       [ -1.0, -2.0, -3.0, -2.0, ]\n",
            "       [ -2.0, -3.0, -2.0, -1.0, ]\n",
            "       [ -3.0, -2.0, -1.0, 0.0, ]\n",
            "]\n",
            "policy -\n",
            "[\n",
            "       [ stay, left, left, left&down, ]\n",
            "       [ up, left&up, right&left&down&up, down, ]\n",
            "       [ up, right&left&down&up, right&down, down, ]\n",
            "       [ right&up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmWnDXbz-Q"
      },
      "source": [
        "## Monte Carlo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI3VPo26ZBP4",
        "outputId": "5bba5c24-8e03-4eb6-836b-b3d06cad90c3"
      },
      "source": [
        "grid = gridworld(size = 4, epochs=30, gamma = 0.8, epsilon_decay_factor = 0.5, seed = 0)\n",
        "grid.gpi_monte_carlo()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #3 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117  0.         -1.         -3.78518437], [-2.74204343 -2.048      -1.4        -3.71357916], [-2.31072    -3.33799881 -1.74666667 -3.63425781], ]\n",
            "       [ [0. 0. 0. 0.], [-3.90890489  0.          0.          0.        ], [ 0.         -1.74666667  0.          0.        ], [-3.0760394   0.          0.         -3.21474836], ]\n",
            "       [ [ 0.          0.         -4.097617   -4.01152922], [-3.96253844  0.          0.          0.        ], [ 0.   0.   0.  -1.4], [ 0. -1.  0.  0.], ]\n",
            "       [ [ 0.         -2.74204343 -2.9194304  -2.048     ], [ 0.          0.         -3.0760394  -3.33799881], [ 0.         -1.4        -3.21474836 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #6 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117 -4.097617   -1.         -3.78518437], [-4.17060197 -2.048      -4.13556779 -3.71357916], [-2.31072    -3.33799881 -1.74666667 -3.63425781], ]\n",
            "       [ [-1.         -4.01152922 -3.21474836 -1.74666667], [-1.4        -4.33415863 -4.05640321  0.        ], [-4.20302231 -1.74666667  0.          0.        ], [-3.0760394   0.          0.         -3.21474836], ]\n",
            "       [ [-3.96253844 -3.54611628 -4.28709592 -4.28709592], [-4.45959979 -4.53491538 -4.31141202 -4.26105026], [-4.23309409 -2.31072    -4.47379352 -1.4       ], [ 0. -1.  0.  0.], ]\n",
            "       [ [-3.44786579 -4.17060197 -4.13556779 -4.51224699], [-4.50006646  0.         -4.52385003 -3.90890489], [ 0.         -2.048      -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, down, left, ]\n",
            "       [ up, right, left, down, ]\n",
            "       [ down, right, right, up, ]\n",
            "       [ up, down, up, stay, ]\n",
            "]\n",
            "epoch #9 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117 -4.097617   -1.         -3.78518437], [-4.17060197 -2.54048    -4.13556779 -3.21474836], [-3.0760394  -3.33799881 -2.74204343 -2.9194304 ], ]\n",
            "       [ [-1.         -4.01152922 -3.21474836 -1.74666667], [-1.4        -2.048      -4.05640321  0.        ], [-4.20302231 -3.63425781 -2.31072     0.        ], [-3.0760394  -3.33799881  0.         -3.21474836], ]\n",
            "       [ [-3.96253844 -3.54611628 -4.28709592 -4.28709592], [-4.45959979 -4.53491538 -4.31141202 -1.74666667], [-4.23309409 -1.4        -2.048      -3.54611628], [-3.44786579 -1.         -2.048       0.        ], ]\n",
            "       [ [-3.44786579 -4.17060197 -4.13556779 -4.51224699], [-4.50006646  0.         -4.52385003 -3.90890489], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, down, left, ]\n",
            "       [ up, right, right, left, ]\n",
            "       [ down, right, down, right, ]\n",
            "       [ up, down, right, stay, ]\n",
            "]\n",
            "epoch #12 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117 -4.097617   -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -3.21474836], [-3.0760394  -3.33799881 -4.88304094 -2.9194304 ], ]\n",
            "       [ [-1.         -4.01152922 -3.21474836 -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -4.8816568 ], [-4.37549518 -3.33799881 -4.88095238 -4.62963179], ]\n",
            "       [ [-3.96253844 -4.85915493 -4.28709592 -4.74358975], [-4.7752809  -4.53491538 -4.86013986 -4.87261146], [-4.23309409 -2.54048    -4.87341772 -3.54611628], [-3.44786579 -1.         -2.048       0.        ], ]\n",
            "       [ [-4.097617   -3.90890489 -4.85815603 -4.85611511], [-4.83333333 -4.85507246 -4.52385003 -3.90890489], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, right, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, down, down, right, ]\n",
            "       [ down, right, right, stay, ]\n",
            "]\n",
            "epoch #15 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117 -4.097617   -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -4.81818182], [-3.0760394  -2.31072    -4.76190476 -4.81651376], ]\n",
            "       [ [-1.         -4.01152922 -3.21474836 -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -4.8816568 ], [-4.37549518 -2.048      -4.88095238 -4.62963179], ]\n",
            "       [ [-1.4        -4.85915493 -4.28709592 -4.74358975], [-4.7752809  -4.53491538 -4.86013986 -4.87261146], [-4.23309409 -1.4        -4.87341772 -3.54611628], [-1.74666667 -1.         -1.74666667 -4.51224699], ]\n",
            "       [ [-1.74666667 -4.41206298 -4.42880323 -4.85611511], [-4.83333333 -4.85507246 -4.52385003 -3.90890489], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, down, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "epoch #18 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117 -4.097617   -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -4.81818182], [-3.0760394  -2.31072    -4.76190476 -4.81651376], ]\n",
            "       [ [-1.         -4.01152922 -1.4        -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -4.8816568 ], [-4.37549518 -2.048      -4.88095238 -4.62963179], ]\n",
            "       [ [-1.4        -4.85915493 -4.28709592 -4.74358975], [-4.7752809  -1.74666667 -4.86013986 -4.87261146], [-4.23309409 -1.4        -4.87341772 -1.4       ], [-1.74666667 -1.         -1.74666667 -4.51224699], ]\n",
            "       [ [-1.74666667 -4.41206298 -4.42880323 -4.85611511], [-4.83333333 -4.85507246 -4.52385003 -1.4       ], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, down, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "epoch #21 [\n",
            "       [ [0. 0. 0. 0.], [-3.85002117 -4.097617   -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -4.81818182], [-3.0760394  -2.31072    -4.76190476 -4.81651376], ]\n",
            "       [ [-1.         -4.01152922 -1.4        -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -4.8816568 ], [-4.37549518 -2.048      -4.88095238 -4.62963179], ]\n",
            "       [ [-1.4        -4.85915493 -4.28709592 -4.74358975], [-1.74666667 -1.74666667 -4.86013986 -4.87261146], [-4.23309409 -1.4        -4.87341772 -1.4       ], [-1.74666667 -1.         -1.74666667 -4.51224699], ]\n",
            "       [ [-1.74666667 -4.41206298 -4.42880323 -4.85611511], [-2.048      -1.74666667 -4.52385003 -1.4       ], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "epoch #24 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -4.097617   -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -4.81818182], [-3.0760394  -2.31072    -4.76190476 -4.81651376], ]\n",
            "       [ [-1.         -4.01152922 -1.4        -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -1.74666667], [-4.37549518 -1.4        -4.88095238 -4.62963179], ]\n",
            "       [ [-1.4        -4.85915493 -4.28709592 -4.74358975], [-1.74666667 -1.74666667 -4.86013986 -4.87261146], [-4.23309409 -1.4        -4.87341772 -1.4       ], [-1.74666667 -1.         -1.74666667 -4.51224699], ]\n",
            "       [ [-1.74666667 -4.41206298 -4.42880323 -4.85611511], [-2.048      -1.74666667 -4.52385003 -1.4       ], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "epoch #27 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -4.097617   -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -4.81818182], [-3.0760394  -2.31072    -4.76190476 -4.81651376], ]\n",
            "       [ [-1.         -4.01152922 -1.4        -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -1.74666667], [-4.37549518 -1.4        -4.88095238 -1.74666667], ]\n",
            "       [ [-1.4        -4.85915493 -4.28709592 -4.74358975], [-1.74666667 -1.74666667 -4.86013986 -4.87261146], [-4.23309409 -1.4        -4.87341772 -1.4       ], [-1.74666667 -1.         -1.74666667 -4.51224699], ]\n",
            "       [ [-1.74666667 -4.41206298 -4.42880323 -4.85611511], [-2.048      -1.74666667 -4.52385003 -1.4       ], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -3.78518437], [-4.17060197 -4.88235294 -4.13556779 -4.81818182], [-3.0760394  -2.31072    -4.76190476 -4.81651376], ]\n",
            "       [ [-1.         -4.01152922 -1.4        -1.74666667], [-1.4        -2.048      -4.05640321 -4.77272727], [-4.61538813 -4.87421384 -4.77011494 -1.74666667], [-4.37549518 -1.4        -4.88095238 -1.74666667], ]\n",
            "       [ [-1.4        -4.85915493 -4.28709592 -2.048     ], [-1.74666667 -1.74666667 -1.74666667 -4.87261146], [-4.23309409 -1.4        -4.87341772 -1.4       ], [-1.74666667 -1.         -1.74666667 -4.51224699], ]\n",
            "       [ [-1.74666667 -4.41206298 -4.42880323 -4.85611511], [-2.048      -1.74666667 -4.52385003 -1.4       ], [-2.31072    -1.4        -3.85002117 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw7IhLpiP2J"
      },
      "source": [
        "## Tester cells (to be ignored)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2coXnct5GI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfa6b75-5336-4e11-fa3e-3804a70e22d2"
      },
      "source": [
        "np.arange(1,15)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIiMNeIid7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
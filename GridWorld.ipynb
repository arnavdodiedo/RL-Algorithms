{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ADkFcuITboaQ",
        "8PFmWnDXbz-Q"
      ],
      "authorship_tag": "ABX9TyMPN9uL3BAhsbzKQ31HvKfO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPTWDQ1btQo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIncFsMcHDI"
      },
      "source": [
        "## Generalized policy iteration with DP, Monte Carlo, TD learning and Q learning\n",
        "#### where the initial policy is a uniform one, equal probabilites of going to each neighbouring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCe0vwSXOv6"
      },
      "source": [
        "class gridworld():\n",
        "    def __init__(self, size = 4, reward = 0, penalty_per_step = -1, epochs = 100, alpha = 0.9, gamma = 0.99, epsilon_decay_factor = 0.99, seed = 0):\n",
        "        self.size = size # square grid world size\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.returngrid = np.zeros((size, size), dtype=np.float) # reward for all states initialised to 0   (state values for each state)     \n",
        "        self.goal_states = [[0,0],[size-1, size-1]] # goal states are the top left and bottom right corners of the grid\n",
        "        self.reward = reward # 0 reward on reaching goal state\n",
        "        self.epsilon = 1\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # terminate on reaching goal state OR stuck in infinite loop with 0 rewards per step in this loop\n",
        "\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # state action value matrix\n",
        "\n",
        "        self.penalty_per_step = penalty_per_step # -1 reward per movement \n",
        "        self.movements = [[-1,0], [1,0], [0,-1], [0,1]]\n",
        "        self.episodes = self.epochs = epochs # number of epochs to run policy evalution\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def reset_grid(self): # reset all agent and environment variables\n",
        "        self.returngrid = np.zeros((self.size, self.size))\n",
        "        self.epsilon = 1\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # reset state action value matrix\n",
        "\n",
        "    def greedy_policy_update_dp(self, curr_state):  # update policy greedily, second step of generalized policy iteration (GPI)\n",
        "        if curr_state not in self.goal_states:\n",
        "            self.policy[curr_state[0]][curr_state[1]] = np.zeros(4)\n",
        "            \n",
        "            up = self.next_state(curr_state, 0)\n",
        "            up = self.returngrid[up[0]][up[1]]\n",
        "\n",
        "            down = self.next_state(curr_state, 1)\n",
        "            down = self.returngrid[down[0]][down[1]]\n",
        "            \n",
        "            left = self.next_state(curr_state, 2)\n",
        "            left = self.returngrid[left[0]][left[1]]\n",
        "            \n",
        "            right = self.next_state(curr_state, 3)\n",
        "            right = self.returngrid[right[0]][right[1]]\n",
        "\n",
        "            arr = [[up, 0], [down, 1], [left, 2], [right, 3]]            \n",
        "            arr.sort()       \n",
        "            arr.reverse()     \n",
        "\n",
        "            m = [arr[0][1]]\n",
        "            for i in range(1, 4):\n",
        "                if arr[i][0] != arr[i-1][0]: break\n",
        "                else: m.append(arr[i][1])\n",
        "            \n",
        "            value = 1/len(m)\n",
        "            \n",
        "            for i in m:\n",
        "                self.policy[curr_state[0]][curr_state[1]][i] = value\n",
        "            \n",
        "            # print(self.policy[curr_state[0]][curr_state[1]])\n",
        "\n",
        "\n",
        "    def next_state(self, curr_state, direction): # direction = 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        change = self.movements[direction]\n",
        "        new_state = [curr_state[0]+change[0], curr_state[1]+change[1]]        \n",
        "\n",
        "        if(new_state[0]<0): new_state[0] = 0\n",
        "        elif(new_state[0]>self.size-1): new_state[0] = self.size-1\n",
        "        \n",
        "        if(new_state[1]<0): new_state[1] = 0\n",
        "        elif(new_state[1]>self.size-1): new_state[1] = self.size-1\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    def display_grid(self): # display the state of the grid world                \n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.returngrid[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_state_action_values(self): # display state action values\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.state_action_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_policy(self): # display policy at each state, for each of the 4 actions\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):                \n",
        "                \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"stay\", end=\", \")\n",
        "                    continue\n",
        "\n",
        "                probs = [[self.policy[i][j][0], 0], [self.policy[i][j][1], 1], [self.policy[i][j][2], 2], [self.policy[i][j][3], 3]]\n",
        "                probs.sort()\n",
        "                probs.reverse()\n",
        "\n",
        "                moves = [\"up\", \"down\", \"left\", \"right\"]\n",
        "                m = [probs[0][1]]\n",
        "\n",
        "                for k in range(1,4):\n",
        "                    if probs[k][0] != probs[k-1][0]: break\n",
        "                    else: m.append(probs[k][1])\n",
        "                \n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end=\"&\")\n",
        "                    else: print(moves[m[k]], end=\", \")\n",
        "                \n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def evaluate_current_policy_dp(self): # policy evaluation using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):                        \n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step + self.gamma * self.returngrid[new_state[0], new_state[1]]) * self.policy[i][j][k]                                \n",
        "                    \n",
        "                    self.returngrid[i][j] = value\n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                \n",
        "    \n",
        "    def gpi_dp(self): # GPI using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):\n",
        "            # either update in self.returngrid as you traverse OR in each epoch maintain a copy of self.returngrid and use it for update at the end\n",
        "            returngrid_copy = np.copy(self.returngrid)\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.gamma * self.returngrid[new_state[0]][new_state[1]]) * self.policy[i][j][k]                    \n",
        "                    returngrid_copy[i,j] = value\n",
        "\n",
        "            if (self.returngrid == returngrid_copy).all():                 \n",
        "                print(\"Policy did not improve. Stopping...\")\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "                break\n",
        "            else: \n",
        "                self.returngrid = returngrid_copy\n",
        "            \n",
        "            for i in range(self.size): \n",
        "                for j in range(self.size): \n",
        "                    self.greedy_policy_update_dp([i,j])            \n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "\n",
        "    def evaluate_current_policy_monte_carlo(self): # evaluate policy using first visit monte carlo\n",
        "        self.reset_grid() # start from scratch\n",
        "\n",
        "        number_of_visits = np.zeros((self.size, self.size))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []\n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]] == 0:\n",
        "                    number_of_visits[state[0],state[1]] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]] = 1\n",
        "                \n",
        "                path.append(state)\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                direction = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                new_state = self.next_state(state, direction)            \n",
        "                state = new_state\n",
        "                \n",
        "            state_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_returns[i] = state_reward[i] + self.gamma * state_returns[i+1]\n",
        "            \n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            for p in range(len(path)):\n",
        "                if is_visited_this_episode[path[p][0]][path[p][1]] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[path[p][0]][path[p][1]] = 1\n",
        "                \n",
        "                self.returngrid[path[p][0]][path[p][1]] += (state_returns[p]-self.returngrid[path[p][0]][path[p][1]])/number_of_visits[path[p][0]][path[p][1]]\n",
        "            \n",
        "            if (_%(self.episodes/10) == 0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                # print(\"policy - \")\n",
        "                # self.display_policy()\n",
        "    \n",
        "    def epsilon_greedy_policy_update_monte_carlo(self): # epsilon greedy update of policy, second step of monte carlo policy improvement\n",
        "        \n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "\n",
        "                if [i,j] in self.goal_states: continue\n",
        "\n",
        "                prob = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)        \n",
        "                prob[np.argmax(self.state_action_values[i][j])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "\n",
        "                self.policy[i][j] = prob\n",
        "            \n",
        "                # print(\"updated policy at\", curr_state, \"to\", self.policy[i][j])\n",
        "\n",
        "    def gpi_monte_carlo(self): # GPI using epsilon greedy first visit monte carlo\n",
        "        self.reset_grid()\n",
        "        number_of_visits = np.zeros((self.size, self.size, 4))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # print(\"epoch\", _)\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            \n",
        "            k = np.random.choice(range(1, self.size*self.size-1))            \n",
        "            i = k//4\n",
        "            j = k - 4*i                                    \n",
        "            \n",
        "            action = np.random.choice([0,1,2,3])\n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []            \n",
        "\n",
        "            while True:\n",
        "                if is_visited_this_episode[state[0]][state[1]][action] == 0:\n",
        "                    number_of_visits[state[0]][state[1]][action] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]][action] = 1\n",
        "\n",
        "                # print(\"in state\", state, \"took action\", action, \"with policy\", self.policy[state[0]][state[1]])                             \n",
        "\n",
        "                path.append([state, action])\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "\n",
        "                # IMPORTANT STEP - first select next state based on current state and action, \n",
        "                # then select next action based on probabilities in this new state\n",
        "                state = self.next_state(state, action)                                \n",
        "\n",
        "                if state in self.goal_states: break\n",
        "        \n",
        "                action = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                \n",
        "            state_action_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_action_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_action_returns[i] = state_reward[i] + self.gamma * state_action_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1   \n",
        "\n",
        "                # self.state_action_values[x][y][a] += (state_action_returns[p]-self.state_action_values[x][y][a])/number_of_visits[x][y][a]\n",
        "                self.state_action_values[x][y][a] = np.mean(state_action_returns[p:])\n",
        "                        \n",
        "            self.epsilon_greedy_policy_update_monte_carlo() # improve the policy based on updated state-action values                                                        \n",
        "\n",
        "            if (_%(self.epochs/10) == 0):                \n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_state_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor   \n",
        "\n",
        "    def evaluate_current_policy_tdlearning(self):\n",
        "        self.reset_grid()\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "\n",
        "            i = k//4\n",
        "            j = k - 4*i\n",
        "            state = [i,j]            \n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                action = np.random.choice([0,1,2,3], p=self.policy[i][j])            \n",
        "                new_state = self.next_state(state, action)\n",
        "                self.returngrid[state[0]][state[1]] += self.alpha * (self.penalty_per_step + self.gamma * self.returngrid[new_state[0]][new_state[1]] - self.returngrid[state[0]][state[1]])\n",
        "                state = new_state\n",
        "\n",
        "            if _%(self.episodes/10)==0:\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkFcuITboaQ"
      },
      "source": [
        "## Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gVHaw87EBs",
        "outputId": "b166246e-5610-4918-e016-0d2f4f2ece7e"
      },
      "source": [
        "grid = gridworld(epochs=100, gamma = 0.9, seed = 0)\n",
        "# grid.evaluate_current_policy_dp()\n",
        "grid.gpi_dp()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy did not improve. Stopping...\n",
            "epoch #4 [\n",
            "       [ 0.0, -1.0, -2.0, -3.0, ]\n",
            "       [ -1.0, -2.0, -3.0, -2.0, ]\n",
            "       [ -2.0, -3.0, -2.0, -1.0, ]\n",
            "       [ -3.0, -2.0, -1.0, 0.0, ]\n",
            "]\n",
            "policy -\n",
            "[\n",
            "       [ stay, left, left, left&down, ]\n",
            "       [ up, left&up, right&left&down&up, down, ]\n",
            "       [ up, right&left&down&up, right&down, down, ]\n",
            "       [ right&up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmWnDXbz-Q"
      },
      "source": [
        "## Monte Carlo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI3VPo26ZBP4",
        "outputId": "3b8a171e-6469-487a-dc1e-3e2df9b62d4e"
      },
      "source": [
        "grid = gridworld(size = 4, epochs=100, gamma = 0.8, epsilon_decay_factor = 0.5, seed = 0)\n",
        "# grid.evaluate_current_policy_monte_carlo()\n",
        "grid.gpi_monte_carlo()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -4.45959979 -1.         -3.0760394 ], [-4.48726469 -4.67741967 -1.4        -4.72222225], [-2.54048    -2.31072    -4.68253993 -4.69230785], ]\n",
            "       [ [-1.         -2.54048    -1.74666667 -4.74025975], [-1.74666667 -3.0760394  -2.74204343 -4.73684212], [-1.74666667 -4.73333335 -3.21474836 -3.44786579], [-4.69696982 -4.71428576 -3.33799881 -3.21474836], ]\n",
            "       [ [-2.048      -4.05640321 -4.74683545 -2.31072   ], [-2.9194304  -3.0760394  -4.75       -3.63425781], [-3.54611628 -4.66666718 -4.62264427 -4.64285848], [-4.71014499 -1.         -4.62963179 -4.63636534], ]\n",
            "       [ [-2.31072    -4.01152922 -2.54048    -3.96253844], [-3.90890489 -2.54048    -2.74204343 -2.9194304 ], [-4.6610176  -2.048      -2.74204343 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, stay, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -4.45959979 -1.         -1.74666667], [-4.48726469 -2.31072    -1.74666667 -2.048     ], [-2.54048    -1.74666667 -4.68253993 -4.69230785], ]\n",
            "       [ [-1.         -2.31072    -2.54048    -4.74025975], [-2.048      -2.54048    -2.74204343 -4.73684212], [-2.54048    -4.73333335 -3.21474836 -3.44786579], [-4.69696982 -1.4        -3.33799881 -3.21474836], ]\n",
            "       [ [-1.4        -4.05640321 -1.74666667 -3.0760394 ], [-2.31072   -3.0760394 -2.048     -2.9194304], [-2.74204343 -4.66666718 -4.62264427 -2.74204343], [-4.71014499 -1.         -2.31072    -2.54048   ], ]\n",
            "       [ [-3.33799881 -4.01152922 -2.54048    -3.96253844], [-3.90890489 -3.71357916 -3.44786579 -2.9194304 ], [-1.74666667 -2.048      -2.74204343 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, left, up, down, ]\n",
            "       [ left, right, right, stay, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -4.45959979 -1.         -1.74666667], [-4.48726469 -2.31072    -1.74666667 -2.048     ], [-2.54048    -1.74666667 -4.68253993 -4.69230785], ]\n",
            "       [ [-1.         -2.31072    -2.54048    -4.74025975], [-1.4        -2.54048    -1.4        -4.73684212], [-2.54048    -4.73333335 -3.21474836 -1.74666667], [-4.69696982 -1.4        -3.33799881 -3.21474836], ]\n",
            "       [ [-1.4        -2.048      -1.74666667 -3.0760394 ], [-1.74666667 -3.0760394  -2.048      -2.31072   ], [-2.048      -1.4        -4.62264427 -2.74204343], [-1.74666667 -1.         -2.31072    -2.54048   ], ]\n",
            "       [ [-1.74666667 -4.01152922 -2.9194304  -3.96253844], [-2.048      -3.71357916 -3.44786579 -2.9194304 ], [-1.74666667 -2.048      -2.74204343 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.74666667 -2.048     ], [-2.54048    -1.74666667 -4.68253993 -4.69230785], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-2.54048    -4.73333335 -3.21474836 -1.74666667], [-2.048      -1.4        -3.33799881 -3.21474836], ]\n",
            "       [ [-1.4        -2.048      -1.74666667 -3.0760394 ], [-1.74666667 -3.0760394  -2.048      -2.31072   ], [-2.048      -1.4        -4.62264427 -2.74204343], [-1.74666667 -1.         -2.31072    -2.54048   ], ]\n",
            "       [ [-1.74666667 -2.048      -2.9194304  -3.96253844], [-2.048      -3.71357916 -3.44786579 -2.9194304 ], [-1.74666667 -2.048      -2.74204343 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.4        -2.048     ], [-2.048      -1.74666667 -1.74666667 -2.048     ], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-2.54048    -4.73333335 -3.21474836 -1.74666667], [-2.048      -1.4        -3.33799881 -3.21474836], ]\n",
            "       [ [-1.4        -2.048      -1.74666667 -3.0760394 ], [-1.74666667 -3.0760394  -1.74666667 -2.31072   ], [-2.048      -1.4        -4.62264427 -2.74204343], [-1.74666667 -1.         -2.31072    -2.54048   ], ]\n",
            "       [ [-1.74666667 -2.31072    -2.048      -3.96253844], [-2.048      -3.71357916 -3.44786579 -2.9194304 ], [-1.74666667 -2.048      -2.74204343 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.4        -2.048     ], [-2.048      -1.74666667 -1.74666667 -2.048     ], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-1.74666667 -1.74666667 -3.21474836 -1.74666667], [-2.048      -1.4        -2.048      -3.21474836], ]\n",
            "       [ [-1.4        -2.048      -1.74666667 -3.0760394 ], [-1.74666667 -3.0760394  -1.74666667 -1.74666667], [-2.048      -1.4        -4.62264427 -1.4       ], [-1.74666667 -1.         -2.31072    -1.4       ], ]\n",
            "       [ [-1.74666667 -2.31072    -2.048      -3.96253844], [-2.048      -3.71357916 -3.44786579 -2.9194304 ], [-1.74666667 -1.4        -2.74204343 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.4        -2.048     ], [-2.048      -1.74666667 -1.74666667 -2.048     ], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-1.74666667 -1.74666667 -3.21474836 -1.74666667], [-2.048      -1.4        -2.048      -3.21474836], ]\n",
            "       [ [-1.4        -2.31072    -1.74666667 -2.048     ], [-1.74666667 -3.0760394  -1.74666667 -1.74666667], [-2.048      -1.4        -4.62264427 -1.4       ], [-1.74666667 -1.         -2.31072    -1.4       ], ]\n",
            "       [ [-1.74666667 -2.048      -2.048      -3.96253844], [-2.048     -2.31072   -2.048     -2.9194304], [-1.74666667 -1.4        -2.31072    -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.4        -2.048     ], [-2.048      -1.74666667 -1.74666667 -2.048     ], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-1.74666667 -1.74666667 -3.21474836 -1.74666667], [-2.048      -1.4        -2.048      -3.21474836], ]\n",
            "       [ [-1.4        -2.31072    -1.74666667 -2.048     ], [-1.74666667 -3.0760394  -1.74666667 -1.74666667], [-2.048 -1.4   -2.048 -1.4  ], [-1.74666667 -1.         -2.31072    -1.4       ], ]\n",
            "       [ [-1.74666667 -2.048      -2.048      -3.96253844], [-2.048     -2.31072   -2.048     -2.9194304], [-1.74666667 -1.4        -2.31072    -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.4        -2.048     ], [-2.048      -1.74666667 -1.74666667 -2.048     ], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-1.74666667 -1.74666667 -1.74666667 -1.74666667], [-2.048      -1.4        -2.048      -1.74666667], ]\n",
            "       [ [-1.4        -2.31072    -1.74666667 -2.048     ], [-1.74666667 -3.0760394  -1.74666667 -1.74666667], [-2.048 -1.4   -2.048 -1.4  ], [-1.74666667 -1.         -2.31072    -1.4       ], ]\n",
            "       [ [-1.74666667 -2.048      -2.048      -3.96253844], [-2.048   -2.31072 -2.048   -1.4    ], [-1.74666667 -1.4        -2.31072    -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ [0. 0. 0. 0.], [-1.4        -1.74666667 -1.         -1.74666667], [-4.48726469 -2.31072    -1.4        -2.048     ], [-2.048      -1.74666667 -1.74666667 -2.048     ], ]\n",
            "       [ [-1.         -1.74666667 -1.4        -4.74025975], [-1.4        -2.048      -1.4        -4.73684212], [-1.74666667 -1.74666667 -1.74666667 -1.74666667], [-2.048      -1.4        -2.048      -1.74666667], ]\n",
            "       [ [-1.4        -2.31072    -1.74666667 -2.048     ], [-1.74666667 -3.0760394  -1.74666667 -1.74666667], [-2.048 -1.4   -2.048 -1.4  ], [-1.74666667 -1.         -2.31072    -1.4       ], ]\n",
            "       [ [-1.74666667 -2.048      -2.048      -3.96253844], [-2.048      -1.74666667 -2.048      -1.4       ], [-1.74666667 -1.4        -2.31072    -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, down, ]\n",
            "       [ up, up, up, down, ]\n",
            "       [ up, up, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEln2xQmrhP3"
      },
      "source": [
        "##TD Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb-E9O6Mrkw5",
        "outputId": "dbe2b789-c530-45d2-f16b-9a30b2a8c9d3"
      },
      "source": [
        "grid = gridworld(size=4, epochs=100, alpha = 0.4, gamma=0.9, seed=0)\n",
        "grid.evaluate_current_policy_tdlearning()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ 0.0, -0.4, 0.0, 0.0, ]\n",
            "       [ -2.9629724360937026, -2.639936981081504, 0.0, 0.0, ]\n",
            "       [ -5.44151317331102, -4.014618934682042, -1.7166933635267365, -0.4, ]\n",
            "       [ -6.166773633673814, -4.727311671708557, -2.3115101357220333, 0.0, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ 0.0, -3.6906582799099343, -6.292470090417065, -6.500122623042976, ]\n",
            "       [ -3.5077005430554573, -6.31259152772494, -6.566089336250388, -6.33824278069204, ]\n",
            "       [ -5.998135981012131, -6.581771664356883, -4.915559162026243, -1.9986345158012115, ]\n",
            "       [ -7.317762953663052, -6.085004646856289, -5.213237152387899, 0.0, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ 0.0, -5.880178946660615, -7.35219747458223, -7.5045626184122245, ]\n",
            "       [ -2.6167744394189896, -6.340465641471173, -7.130925611995167, -5.888951729411248, ]\n",
            "       [ -7.250678064283298, -7.189584699331956, -6.797452462621809, -3.614429250769203, ]\n",
            "       [ -7.782966144868626, -6.818831956791831, -3.3627339164741805, 0.0, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ 0.0, -4.844181901626105, -7.744939139275797, -7.916228246988059, ]\n",
            "       [ -4.971699733080612, -6.601481055375683, -6.6487287336997305, -7.04679370988137, ]\n",
            "       [ -5.878372057001802, -6.534680201081627, -5.103215530887098, -6.349752806044159, ]\n",
            "       [ -7.057070427537211, -6.490203706545806, -4.458337665232639, 0.0, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ 0.0, -6.710815033211553, -6.894401369396651, -8.088713930154489, ]\n",
            "       [ -4.347789517666648, -7.0407421938079455, -6.251824054901412, -7.17719251424549, ]\n",
            "       [ -7.0583890972577015, -6.866725711942888, -6.2984287338515434, -4.6302872582576455, ]\n",
            "       [ -7.179597691693744, -5.46767808372083, -2.404787727487341, 0.0, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ 0.0, -3.827045651496853, -7.244194177185483, -7.733710366184942, ]\n",
            "       [ -6.995588873447186, -6.688051876181542, -7.761245796830943, -8.349785678719904, ]\n",
            "       [ -6.974032226363739, -6.705813592818238, -6.176286836889591, -7.160742030421436, ]\n",
            "       [ -6.773529340910786, -5.24381218909238, -2.165024017539532, 0.0, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ 0.0, -4.139017934134728, -8.377388514697152, -8.500467824571382, ]\n",
            "       [ -4.2899237368621765, -7.578056135707446, -8.354836146886889, -7.873098338513429, ]\n",
            "       [ -7.382300869027415, -7.36011258116839, -7.218113108992989, -4.632748954450218, ]\n",
            "       [ -7.588629616868737, -7.558267798086292, -4.61300454959362, 0.0, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ 0.0, -7.81234289503057, -8.21924239208209, -8.652202136435566, ]\n",
            "       [ -5.1284132629114225, -8.045525457874126, -7.004778526600443, -7.648640778607739, ]\n",
            "       [ -7.935571874491101, -6.967085818830014, -5.76630283503324, -4.405555669499254, ]\n",
            "       [ -7.640078336456421, -7.587839049200903, -5.2023310408183985, 0.0, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ 0.0, -4.246859805957811, -7.2521315713719074, -7.964689550753896, ]\n",
            "       [ -3.170098147093424, -6.865766450554156, -8.265819371269552, -7.939036847994827, ]\n",
            "       [ -7.6913119102153065, -7.761320036094515, -7.742779644914741, -4.325674418466876, ]\n",
            "       [ -8.052872652759818, -7.227077115912948, -2.3700726413958995, 0.0, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ 0.0, -5.218895580386905, -6.982436247550111, -8.208336689709357, ]\n",
            "       [ -4.489311366194151, -7.083783749832388, -6.905078159758922, -7.5891630086025845, ]\n",
            "       [ -8.112200668965475, -7.338081729706917, -5.838704843853185, -3.3254142643790514, ]\n",
            "       [ -8.463378491859556, -7.226742085952345, -5.236440797853145, 0.0, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw7IhLpiP2J"
      },
      "source": [
        "## Tester cells (to be ignored)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2coXnct5GI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfa6b75-5336-4e11-fa3e-3804a70e22d2"
      },
      "source": [
        "np.arange(1,15)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIiMNeIid7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
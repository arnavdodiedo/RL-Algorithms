{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ADkFcuITboaQ",
        "8PFmWnDXbz-Q",
        "3sw7IhLpiP2J"
      ],
      "authorship_tag": "ABX9TyM5C3BQa8fiBA5Izta2xLiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPTWDQ1btQo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIncFsMcHDI"
      },
      "source": [
        "## Generalized policy iteration with DP, Monte Carlo, TD learning and Q learning\n",
        "#### where the initial policy is a uniform one, equal probabilites of going to each neighbouring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCe0vwSXOv6"
      },
      "source": [
        "class gridworld():\n",
        "    def __init__(self, size = 4, reward = 0, penalty_per_step = -1, episodes = 100, alpha = 0.9, gamma = 0.99, epsilon_decay_factor = 0.99, seed = 0):\n",
        "        self.size = size # square grid world size\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.state_values = np.zeros((size, size), dtype=np.float) # reward for all states initialised to 0   (state values for each state)     \n",
        "        self.goal_states = [[0,0],[size-1, size-1]] # goal states are the top left and bottom right corners of the grid\n",
        "        self.reward = reward # 0 reward on reaching goal state\n",
        "        self.epsilon = 1\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # terminate on reaching goal state OR stuck in infinite loop with 0 rewards per step in this loop\n",
        "\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "\n",
        "        self.action_values = np.zeros((self.size, self.size, 4)) # state action value matrix\n",
        "\n",
        "        self.penalty_per_step = penalty_per_step # -1 reward per movement \n",
        "        self.movements = [[-1,0], [1,0], [0,-1], [0,1]]\n",
        "        self.episodes = episodes # number of episodes to run policy evalution\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def reset_grid(self): # reset all agent and environment variables\n",
        "        self.state_values = np.zeros((self.size, self.size))\n",
        "        self.epsilon = 1\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "        self.action_values = np.zeros((self.size, self.size, 4)) # reset state action value matrix\n",
        "\n",
        "    def greedy_policy_update_dp(self, curr_state):  # update policy greedily, second step of generalized policy iteration (GPI)\n",
        "        if curr_state not in self.goal_states:\n",
        "            self.policy[curr_state[0]][curr_state[1]] = np.zeros(4)\n",
        "            \n",
        "            up = self.next_state(curr_state, 0)\n",
        "            up = self.state_values[up[0]][up[1]]\n",
        "\n",
        "            down = self.next_state(curr_state, 1)\n",
        "            down = self.state_values[down[0]][down[1]]\n",
        "            \n",
        "            left = self.next_state(curr_state, 2)\n",
        "            left = self.state_values[left[0]][left[1]]\n",
        "            \n",
        "            right = self.next_state(curr_state, 3)\n",
        "            right = self.state_values[right[0]][right[1]]\n",
        "\n",
        "            arr = [[up, 0], [down, 1], [left, 2], [right, 3]]            \n",
        "            arr.sort()       \n",
        "            arr.reverse()     \n",
        "\n",
        "            m = [arr[0][1]]\n",
        "            for i in range(1, 4):\n",
        "                if arr[i][0] != arr[i-1][0]: break\n",
        "                else: m.append(arr[i][1])\n",
        "            \n",
        "            value = 1/len(m)\n",
        "            \n",
        "            for i in m:\n",
        "                self.policy[curr_state[0]][curr_state[1]][i] = value\n",
        "            \n",
        "            # print(self.policy[curr_state[0]][curr_state[1]])\n",
        "\n",
        "\n",
        "    def next_state(self, curr_state, direction): # direction = 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        change = self.movements[direction]\n",
        "        new_state = [curr_state[0]+change[0], curr_state[1]+change[1]]        \n",
        "\n",
        "        if(new_state[0]<0): new_state[0] = 0\n",
        "        elif(new_state[0]>self.size-1): new_state[0] = self.size-1\n",
        "        \n",
        "        if(new_state[1]<0): new_state[1] = 0\n",
        "        elif(new_state[1]>self.size-1): new_state[1] = self.size-1\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    def display_grid(self): # display the state of the grid world                \n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.state_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_action_values(self): # display state action values\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.action_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_policy(self): # display policy at each state, for each of the 4 actions\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):                \n",
        "                \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"stay\", end=\", \")\n",
        "                    continue\n",
        "\n",
        "                probs = [[self.policy[i][j][0], 0], [self.policy[i][j][1], 1], [self.policy[i][j][2], 2], [self.policy[i][j][3], 3]]\n",
        "                probs.sort()\n",
        "                probs.reverse()\n",
        "\n",
        "                moves = [\"up\", \"down\", \"left\", \"right\"]\n",
        "                m = [probs[0][1]]\n",
        "\n",
        "                for k in range(1,4):\n",
        "                    if probs[k][0] != probs[k-1][0]: break\n",
        "                    else: m.append(probs[k][1])\n",
        "                \n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end=\"&\")\n",
        "                    else: print(moves[m[k]], end=\", \")\n",
        "                \n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def evaluate_current_policy_dp(self): # policy evaluation using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.episodes):                        \n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step + self.gamma * self.state_values[new_state[0], new_state[1]]) * self.policy[i][j][k]                                \n",
        "                    \n",
        "                    self.state_values[i][j] = value\n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                \n",
        "    \n",
        "    def gpi_dp(self): # GPI using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # either update in self.state_values as you traverse OR in each epoch maintain a copy of self.state_values and use it for update at the end\n",
        "            state_values_copy = np.copy(self.state_values)\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.gamma * self.state_values[new_state[0]][new_state[1]]) * self.policy[i][j][k]                    \n",
        "                    state_values_copy[i,j] = value\n",
        "\n",
        "            if (self.state_values == state_values_copy).all():                 \n",
        "                print(\"Policy did not improve. Stopping...\")\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "                break\n",
        "            else: \n",
        "                self.state_values = state_values_copy\n",
        "            \n",
        "            for i in range(self.size): \n",
        "                for j in range(self.size): \n",
        "                    self.greedy_policy_update_dp([i,j])            \n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "\n",
        "    def evaluate_current_policy_monte_carlo(self): # evaluate policy using first visit monte carlo\n",
        "        self.reset_grid() # start from scratch\n",
        "\n",
        "        number_of_visits = np.zeros((self.size, self.size))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []\n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]] == 0:\n",
        "                    number_of_visits[state[0],state[1]] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]] = 1\n",
        "                \n",
        "                path.append(state)\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                direction = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                new_state = self.next_state(state, direction)            \n",
        "                state = new_state\n",
        "                \n",
        "            state_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_returns[i] = state_reward[i] + self.gamma * state_returns[i+1]\n",
        "            \n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            for p in range(len(path)):\n",
        "                if is_visited_this_episode[path[p][0]][path[p][1]] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[path[p][0]][path[p][1]] = 1\n",
        "                \n",
        "                self.state_values[path[p][0]][path[p][1]] += (state_returns[p]-self.state_values[path[p][0]][path[p][1]])/number_of_visits[path[p][0]][path[p][1]]\n",
        "            \n",
        "            if (_%(self.episodes/10) == 0):\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                # print(\"policy - \")\n",
        "                # self.display_policy()\n",
        "    \n",
        "    def epsilon_greedy_policy_update_monte_carlo(self): # epsilon greedy update of policy, second step of monte carlo policy improvement\n",
        "        \n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "\n",
        "                if [i,j] in self.goal_states: continue\n",
        "\n",
        "                prob = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)        \n",
        "                prob[np.argmax(self.action_values[i][j])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "\n",
        "                self.policy[i][j] = prob\n",
        "            \n",
        "                # print(\"updated policy at\", curr_state, \"to\", self.policy[i][j])\n",
        "\n",
        "    def gpi_monte_carlo(self): # GPI using epsilon greedy first visit monte carlo\n",
        "        self.reset_grid()\n",
        "        number_of_visits = np.zeros((self.size, self.size, 4))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # print(\"epoch\", _)\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            \n",
        "            k = np.random.choice(range(1, self.size*self.size-1))            \n",
        "            i = k//4\n",
        "            j = k - 4*i                                    \n",
        "            \n",
        "            action = np.random.choice([0,1,2,3])\n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []            \n",
        "\n",
        "            while True:\n",
        "                if is_visited_this_episode[state[0]][state[1]][action] == 0:\n",
        "                    number_of_visits[state[0]][state[1]][action] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]][action] = 1\n",
        "\n",
        "                # print(\"in state\", state, \"took action\", action, \"with policy\", self.policy[state[0]][state[1]])                             \n",
        "\n",
        "                path.append([state, action])\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "\n",
        "                # IMPORTANT STEP - first select next state based on current state and action, \n",
        "                # then select next action based on probabilities in this new state\n",
        "                state = self.next_state(state, action)                                \n",
        "\n",
        "                if state in self.goal_states: break\n",
        "        \n",
        "                action = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                \n",
        "            state_action_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_action_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_action_returns[i] = state_reward[i] + self.gamma * state_action_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1   \n",
        "\n",
        "                # self.action_values[x][y][a] += (state_action_returns[p]-self.action_values[x][y][a])/number_of_visits[x][y][a]\n",
        "                self.action_values[x][y][a] = np.mean(state_action_returns[p:])\n",
        "                        \n",
        "            self.epsilon_greedy_policy_update_monte_carlo() # improve the policy based on updated state-action values                                                        \n",
        "\n",
        "            if (_%(self.episodes/10) == 0):                \n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor   \n",
        "\n",
        "    def evaluate_current_policy_tdlearning(self): # evaluate policy using TD(0) learning\n",
        "        self.reset_grid()\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i\n",
        "            state = [i,j]            \n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                action = np.random.choice([0,1,2,3], p=self.policy[i][j])            \n",
        "                new_state = self.next_state(state, action)\n",
        "                self.state_values[state[0]][state[1]] += self.alpha * (self.penalty_per_step + self.gamma * self.state_values[new_state[0]][new_state[1]] - self.state_values[state[0]][state[1]])\n",
        "                state = new_state\n",
        "\n",
        "            if _%(self.episodes/10)==0:\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "\n",
        "    def sarsa(self): # on-policy TD learning GPI\n",
        "        self.reset_grid()\n",
        "\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            \n",
        "            k = np.random.choice(range(1,15))\n",
        "            i= k//4\n",
        "            j = k - 4*i\n",
        "            state = [i,j]\n",
        "\n",
        "            pr = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)\n",
        "            pr[np.argmax(self.action_values[state[0]][state[1]])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "            self.policy[state[0]][state[1]] = pr\n",
        "            action = np.random.choice([0,1,2,3], p=pr)\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                new_state = self.next_state(state, action)\n",
        "\n",
        "                pr = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)\n",
        "                pr[np.argmax(self.action_values[new_state[0]][new_state[1]])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "                self.policy[new_state[0]][new_state[1]] = pr\n",
        "                new_action = np.random.choice([0,1,2,3], p=pr)\n",
        "\n",
        "                self.action_values[state[0]][state[1]][action] += self.alpha * (self.penalty_per_step + self.gamma * self.action_values[new_state[0]][new_state[1]][new_action] - self.action_values[state[0]][state[1]][action])\n",
        "                \n",
        "                state = new_state\n",
        "                action = new_action\n",
        "            \n",
        "            if _%(self.episodes/10)==0:\n",
        "                print(\"episode #%d\"%(_), end=\" \")\n",
        "                self.display_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()  \n",
        "\n",
        "                self.epsilon *= self.epsilon_decay_factor                  \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkFcuITboaQ"
      },
      "source": [
        "## Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gVHaw87EBs",
        "outputId": "cb8430de-6679-4eb7-ea42-2f28b30a3115"
      },
      "source": [
        "grid = gridworld(episodes=100, gamma = 0.9, seed = 0)\n",
        "# grid.evaluate_current_policy_dp()\n",
        "grid.gpi_dp()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy did not improve. Stopping...\n",
            "epoch #4 [\n",
            "       [ 0.0, -1.0, -1.9, -2.71, ]\n",
            "       [ -1.0, -1.9, -2.71, -1.9, ]\n",
            "       [ -1.9, -2.71, -1.9, -1.0, ]\n",
            "       [ -2.71, -1.9, -1.0, 0.0, ]\n",
            "]\n",
            "policy -\n",
            "[\n",
            "       [ stay, left, left, left&down, ]\n",
            "       [ up, left&up, right&left&down&up, down, ]\n",
            "       [ up, right&left&down&up, right&down, down, ]\n",
            "       [ right&up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmWnDXbz-Q"
      },
      "source": [
        "## Monte Carlo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI3VPo26ZBP4",
        "outputId": "168074e6-7bc8-457d-f001-d04d5166052a"
      },
      "source": [
        "grid = gridworld(size = 4, episodes=100, gamma = 0.8, epsilon_decay_factor = 0.5, seed = 0)\n",
        "# grid.evaluate_current_policy_monte_carlo()\n",
        "grid.gpi_monte_carlo()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ 0.0, -1.0, 0.0, 0.0, ]\n",
            "       [ -4.407095119842893, -3.808101121192415, 0.0, 0.0, ]\n",
            "       [ -4.446367306568033, -3.6384079861977523, -2.4926258176, -1.0, ]\n",
            "       [ -4.251987300034684, -4.250054426331155, -2.0069356013308197, 0.0, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ 0.0, -3.9293197550788266, -4.601437166247207, -4.949477169613706, ]\n",
            "       [ -3.947522471583098, -4.193485617650129, -4.533937734713759, -4.946706484359731, ]\n",
            "       [ -4.415114830578479, -4.01102157938329, -3.6428574751720197, -2.63521036244089, ]\n",
            "       [ -4.408674003501692, -4.380777322322286, -3.06094981755488, 0.0, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ 0.0, -4.080019386565007, -4.514692520695907, -4.961836924633047, ]\n",
            "       [ -3.8492170891980098, -4.307672151048828, -4.6477970671303135, -4.948882604618631, ]\n",
            "       [ -4.299468431056001, -4.20161687957188, -3.968415019798933, -2.9616083389209225, ]\n",
            "       [ -4.508985762918892, -4.490389328936819, -3.008555051464264, 0.0, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ 0.0, -4.052700213895632, -4.42125905102205, -4.790243979230169, ]\n",
            "       [ -3.7489483609424727, -4.150186894098453, -4.454185724846272, -4.814216668887388, ]\n",
            "       [ -4.299364329610695, -4.31255306508692, -3.9646022024637366, -3.398840528468098, ]\n",
            "       [ -4.392226055194314, -4.4171035136688355, -3.1237255870160623, 0.0, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ 0.0, -4.035467869146636, -4.532617536122994, -4.800022337040892, ]\n",
            "       [ -3.76045808125144, -4.182205259262917, -4.523598254760393, -4.760640147979186, ]\n",
            "       [ -4.24926075761077, -4.318682585942697, -3.9632001140602355, -3.384387245524823, ]\n",
            "       [ -4.422070380996887, -4.238697546465013, -2.945275956695077, 0.0, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ 0.0, -3.9099065536993383, -4.482231625074508, -4.748980588639893, ]\n",
            "       [ -3.7978954043760322, -4.104938040298183, -4.497880816881052, -4.701535540181763, ]\n",
            "       [ -4.243502132984537, -4.361169687365901, -3.861681658839727, -3.4758384969018516, ]\n",
            "       [ -4.346313231827578, -4.213548084598056, -2.8698412570382517, 0.0, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ 0.0, -3.813981792099114, -4.541418972901883, -4.7107688633114515, ]\n",
            "       [ -3.71764762841019, -4.185720510022753, -4.602179467282952, -4.742889445115133, ]\n",
            "       [ -4.210441339427358, -4.440092736417489, -4.034678194342107, -3.9057586496434875, ]\n",
            "       [ -4.405754353248604, -4.295223456658515, -3.053786794849428, 0.0, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ 0.0, -3.8836310559664975, -4.532066230651834, -4.699526244705008, ]\n",
            "       [ -3.7688724116473287, -4.231805522910184, -4.594644501536869, -4.6499763866418915, ]\n",
            "       [ -4.205468291980692, -4.494393918518598, -4.086072633571363, -3.6683355974519425, ]\n",
            "       [ -4.4619607351801935, -4.320751099995259, -3.03382287008124, 0.0, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ 0.0, -3.902469072144679, -4.541887817478309, -4.711976731001692, ]\n",
            "       [ -3.703457070082439, -4.212039959193038, -4.630815369206201, -4.599135171048834, ]\n",
            "       [ -4.211904660371375, -4.460759949489107, -4.135669834197998, -3.694282579147083, ]\n",
            "       [ -4.4586016692663835, -4.342043466805112, -3.0437298500020917, 0.0, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ 0.0, -3.822115906456238, -4.567423418423555, -4.698148016635028, ]\n",
            "       [ -3.6837295848701324, -4.224839305363805, -4.618141058615165, -4.619704370063106, ]\n",
            "       [ -4.240828381293138, -4.468306593140569, -4.122913849326781, -3.7072171311085174, ]\n",
            "       [ -4.490068585259431, -4.394890839965273, -3.054462218793125, 0.0, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEln2xQmrhP3"
      },
      "source": [
        "##TD Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb-E9O6Mrkw5",
        "outputId": "ca93b101-112b-4e3f-d024-bf3b8e33adbb"
      },
      "source": [
        "grid = gridworld(size=4, episodes=100, alpha = 0.4, gamma=0.9, seed=0)\n",
        "grid.evaluate_current_policy_tdlearning()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ 0.0, -0.4, 0.0, 0.0, ]\n",
            "       [ -2.9629724360937026, -2.639936981081504, 0.0, 0.0, ]\n",
            "       [ -5.44151317331102, -4.014618934682042, -1.7166933635267365, -0.4, ]\n",
            "       [ -6.166773633673814, -4.727311671708557, -2.3115101357220333, 0.0, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ 0.0, -3.6906582799099343, -6.292470090417065, -6.500122623042976, ]\n",
            "       [ -3.5077005430554573, -6.31259152772494, -6.566089336250388, -6.33824278069204, ]\n",
            "       [ -5.998135981012131, -6.581771664356883, -4.915559162026243, -1.9986345158012115, ]\n",
            "       [ -7.317762953663052, -6.085004646856289, -5.213237152387899, 0.0, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ 0.0, -5.880178946660615, -7.35219747458223, -7.5045626184122245, ]\n",
            "       [ -2.6167744394189896, -6.340465641471173, -7.130925611995167, -5.888951729411248, ]\n",
            "       [ -7.250678064283298, -7.189584699331956, -6.797452462621809, -3.614429250769203, ]\n",
            "       [ -7.782966144868626, -6.818831956791831, -3.3627339164741805, 0.0, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ 0.0, -4.844181901626105, -7.744939139275797, -7.916228246988059, ]\n",
            "       [ -4.971699733080612, -6.601481055375683, -6.6487287336997305, -7.04679370988137, ]\n",
            "       [ -5.878372057001802, -6.534680201081627, -5.103215530887098, -6.349752806044159, ]\n",
            "       [ -7.057070427537211, -6.490203706545806, -4.458337665232639, 0.0, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ 0.0, -6.710815033211553, -6.894401369396651, -8.088713930154489, ]\n",
            "       [ -4.347789517666648, -7.0407421938079455, -6.251824054901412, -7.17719251424549, ]\n",
            "       [ -7.0583890972577015, -6.866725711942888, -6.2984287338515434, -4.6302872582576455, ]\n",
            "       [ -7.179597691693744, -5.46767808372083, -2.404787727487341, 0.0, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ 0.0, -3.827045651496853, -7.244194177185483, -7.733710366184942, ]\n",
            "       [ -6.995588873447186, -6.688051876181542, -7.761245796830943, -8.349785678719904, ]\n",
            "       [ -6.974032226363739, -6.705813592818238, -6.176286836889591, -7.160742030421436, ]\n",
            "       [ -6.773529340910786, -5.24381218909238, -2.165024017539532, 0.0, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ 0.0, -4.139017934134728, -8.377388514697152, -8.500467824571382, ]\n",
            "       [ -4.2899237368621765, -7.578056135707446, -8.354836146886889, -7.873098338513429, ]\n",
            "       [ -7.382300869027415, -7.36011258116839, -7.218113108992989, -4.632748954450218, ]\n",
            "       [ -7.588629616868737, -7.558267798086292, -4.61300454959362, 0.0, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ 0.0, -7.81234289503057, -8.21924239208209, -8.652202136435566, ]\n",
            "       [ -5.1284132629114225, -8.045525457874126, -7.004778526600443, -7.648640778607739, ]\n",
            "       [ -7.935571874491101, -6.967085818830014, -5.76630283503324, -4.405555669499254, ]\n",
            "       [ -7.640078336456421, -7.587839049200903, -5.2023310408183985, 0.0, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ 0.0, -4.246859805957811, -7.2521315713719074, -7.964689550753896, ]\n",
            "       [ -3.170098147093424, -6.865766450554156, -8.265819371269552, -7.939036847994827, ]\n",
            "       [ -7.6913119102153065, -7.761320036094515, -7.742779644914741, -4.325674418466876, ]\n",
            "       [ -8.052872652759818, -7.227077115912948, -2.3700726413958995, 0.0, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ 0.0, -5.218895580386905, -6.982436247550111, -8.208336689709357, ]\n",
            "       [ -4.489311366194151, -7.083783749832388, -6.905078159758922, -7.5891630086025845, ]\n",
            "       [ -8.112200668965475, -7.338081729706917, -5.838704843853185, -3.3254142643790514, ]\n",
            "       [ -8.463378491859556, -7.226742085952345, -5.236440797853145, 0.0, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2WArJLPDRDw"
      },
      "source": [
        "### SARSA On-policy TD control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNJtF4USDQR5",
        "outputId": "d40122a3-56d4-4f44-dbe2-007a7fa3cd46"
      },
      "source": [
        "grid = gridworld(size=4, episodes=100, alpha=0.6, gamma=0.9, epsilon_decay_factor=0.9, seed=0)\n",
        "grid.sarsa()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode #10 [\n",
            "       [ [0. 0. 0. 0.], [-1.73753636 -2.90246222 -0.84       -1.66944106], [-1.0656     -2.43776907 -1.0536     -1.9622112 ], [-1.3783776  -1.805136   -1.88379072 -1.601664  ], ]\n",
            "       [ [ 0.         -3.36776692 -1.9369272  -2.47295943], [-2.36194128 -4.01072865 -2.64095664 -2.93739556], [-2.02884096 -2.98814258 -3.64958835 -3.21957975], [-0.84       -1.57104    -3.51638361 -0.936     ], ]\n",
            "       [ [-1.51672595 -3.12444722 -3.05327265 -3.53044584], [-2.71644937 -2.37115944 -2.97987341 -2.17875465], [-2.43209232 -0.6        -3.55071167 -1.77045504], [ 0.         -0.9744     -2.15298858 -0.6       ], ]\n",
            "       [ [-2.28182225 -2.71943528 -1.89348175 -2.5859292 ], [-2.9273422  -3.08449283 -0.6        -2.08330155], [-1.9373543  -0.924      -2.76609963 -0.9744    ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, right&left&down&up, ]\n",
            "       [ right&left&down&up, right&left&down&up, right&left&down&up, stay, ]\n",
            "]\n",
            "episode #20 [\n",
            "       [ [0. 0. 0. 0.], [-2.33815747 -3.18710147 -0.9744     -2.21110056], [-2.30045528 -3.45528252 -2.32813571 -2.40876173], [-1.3783776  -2.17935936 -3.07541678 -2.25791259], ]\n",
            "       [ [-0.6        -3.72511316 -1.56828463 -2.47295943], [-3.51021492 -3.9485442  -2.70232334 -3.19386849], [-3.53441187 -2.42243731 -3.93176641 -3.21957975], [-1.92284698 -2.35784437 -3.46919067 -2.65571822], ]\n",
            "       [ [-3.02528452 -3.24618065 -3.68600365 -3.91417244], [-3.57420162 -3.2140899  -3.50514023 -2.42754758], [-3.86148622 -1.66176    -3.62941631 -1.87887834], [-2.79301566 -0.99973786 -2.67049647 -1.91809785], ]\n",
            "       [ [-2.28182225 -2.71943528 -1.89348175 -3.38279423], [-3.59595974 -3.41456192 -0.6        -2.47949194], [-2.7983087  -2.74745605 -2.83142269 -0.995904  ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, up, ]\n",
            "       [ up, left, down, up, ]\n",
            "       [ up, right, down, down, ]\n",
            "       [ left, left, right, stay, ]\n",
            "]\n",
            "episode #30 [\n",
            "       [ [0. 0. 0. 0.], [-2.06143899 -3.20040687 -0.98976    -2.21110056], [-2.82091344 -3.29022916 -3.25228908 -2.74240424], [-3.01899504 -3.3451067  -3.07639415 -2.68001909], ]\n",
            "       [ [-0.936      -3.7236989  -3.23887496 -3.75929467], [-3.51021492 -3.94166326 -3.19146302 -3.64386473], [-3.77423577 -2.86083677 -4.30492043 -3.21957975], [-1.92284698 -2.35784437 -4.11083013 -2.65571822], ]\n",
            "       [ [-2.31180476 -3.95490811 -3.68600365 -3.7480517 ], [-4.09273764 -3.40365245 -3.50514023 -3.15672708], [-3.86148622 -2.27963215 -3.93579156 -2.58681326], [-2.79301566 -0.99999329 -2.68279289 -2.27966553], ]\n",
            "       [ [-3.89388181 -4.45223958 -4.4013539  -3.79698113], [-3.77964717 -4.06569354 -3.80070812 -3.03207723], [-3.65615871 -2.74745605 -2.05656908 -0.99973786], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, right, down, ]\n",
            "       [ up, left, down, up, ]\n",
            "       [ up, down, down, down, ]\n",
            "       [ right, left, right, stay, ]\n",
            "]\n",
            "episode #40 [\n",
            "       [ [0. 0. 0. 0.], [-2.06143899 -3.20040687 -0.98976    -2.21110056], [-3.42621826 -3.29022916 -3.25228908 -3.48792612], [-3.79134701 -3.20744439 -3.35385092 -4.02045577], ]\n",
            "       [ [-0.9744     -3.99865999 -3.23887496 -4.07140482], [-3.51021492 -3.94166326 -3.19146302 -3.60239775], [-3.77423577 -3.18701359 -4.30492043 -3.21957975], [-3.86591163 -1.9921922  -3.85101353 -3.83891636], ]\n",
            "       [ [-2.72313074 -3.95490811 -4.25828395 -3.7480517 ], [-4.09273764 -4.09436982 -3.25043066 -3.15672708], [-3.86148622 -2.0517113  -3.93579156 -2.17472521], [-3.17554358 -0.99999993 -2.68279289 -2.34803424], ]\n",
            "       [ [-4.14799469 -4.75762694 -4.38884321 -4.33371908], [-4.0727082  -4.06569354 -4.51249323 -1.97242576], [-3.65615871 -2.74745605 -2.05656908 -0.99999329], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, left, ]\n",
            "       [ up, left, down, down, ]\n",
            "       [ left, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #50 [\n",
            "       [ [0. 0. 0. 0.], [-2.18521612 -3.82545753 -0.9983616  -3.89565414], [-3.59809735 -4.2407487  -3.95626111 -4.46086405], [-4.28758492 -4.43878311 -3.44534731 -4.34394072], ]\n",
            "       [ [-0.995904   -3.99865999 -4.09410859 -4.19952126], [-3.41811371 -3.94166326 -3.93096985 -4.28927201], [-4.39181892 -3.98931024 -3.66164191 -3.96737921], [-4.19369204 -3.26934802 -4.52641292 -3.21135033], ]\n",
            "       [ [-2.72313074 -3.95490811 -4.25828395 -3.7480517 ], [-4.09273764 -3.6410726  -3.25043066 -3.15672708], [-3.88316755 -2.1524925  -4.69854353 -3.08336789], [-4.01127937 -0.99999997 -2.84746877 -3.50996227], ]\n",
            "       [ [-4.14799469 -4.90727896 -4.59633158 -3.64903497], [-4.0727082  -4.06569354 -5.30018183 -3.44193673], [-4.14766604 -2.41304203 -3.8319751  -0.99999993], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, left, ]\n",
            "       [ up, up, right, right, ]\n",
            "       [ left, right, down, down, ]\n",
            "       [ up, right, right, stay, ]\n",
            "]\n",
            "episode #60 [\n",
            "       [ [0. 0. 0. 0.], [-2.18521612 -3.28696583 -0.99989514 -3.89565414], [-4.32926561 -4.45052701 -4.24825151 -4.46086405], [-4.28758492 -4.43878311 -3.9211115  -4.34394072], ]\n",
            "       [ [-0.99934464 -3.99865999 -4.09410859 -4.19952126], [-1.9967346  -3.94166326 -3.93096985 -4.45809357], [-4.64673187 -4.41671023 -3.66164191 -4.55742206], [-4.13796436 -2.60355225 -4.52641292 -4.40958816], ]\n",
            "       [ [-2.22704046 -3.95490811 -4.25828395 -3.7480517 ], [-4.55330194 -3.6410726  -3.37066286 -3.86117126], [-4.52484924 -2.14726816 -4.40973955 -4.0640067 ], [-4.23443082 -0.99999999 -4.11845977 -4.4905455 ], ]\n",
            "       [ [-4.14799469 -4.90727896 -4.59633158 -4.25887642], [-3.73438502 -4.06569354 -5.30018183 -4.26415448], [-3.70220278 -3.80495647 -3.99143587 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, down, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, left, down, down, ]\n",
            "       [ right, right, right, stay, ]\n",
            "]\n",
            "episode #70 [\n",
            "       [ [0. 0. 0. 0.], [-2.01408282 -3.38601374 -0.99999893 -4.45231747], [-4.32926561 -4.45052701 -2.83927796 -4.46086405], [-4.28758492 -4.43878311 -3.9211115  -4.34394072], ]\n",
            "       [ [-0.99973786 -3.99865999 -4.09410859 -4.19952126], [-2.32648036 -3.87813348 -3.93096985 -4.45809357], [-4.64673187 -4.41671023 -3.09899751 -4.55742206], [-4.13796436 -2.1814209  -4.3878518  -3.90392336], ]\n",
            "       [ [-2.03046229 -4.42188038 -4.3272615  -4.06539988], [-4.55330194 -4.35907246 -3.99599718 -3.86117126], [-4.52484924 -3.4542394  -4.40973955 -4.0640067 ], [-4.23443082 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -4.90727896 -4.59633158 -4.25887642], [-3.73438502 -4.06569354 -5.30018183 -4.30485129], [-3.16029104 -2.66198259 -3.99143587 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, down, down, down, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "episode #80 [\n",
            "       [ [0. 0. 0. 0.], [-2.01408282 -3.38601374 -1.         -4.45231747], [-4.32926561 -4.88944601 -1.92404543 -4.8843675 ], [-4.28758492 -4.43878311 -3.29633938 -4.34394072], ]\n",
            "       [ [-0.99989514 -3.99865999 -4.09410859 -4.19952126], [-1.96823681 -3.87813348 -3.93096985 -4.45809357], [-3.68757647 -3.86010954 -2.95771871 -4.55742206], [-4.13796436 -2.1814209  -4.16952435 -4.97209242], ]\n",
            "       [ [-1.95204336 -5.01868279 -4.3272615  -4.06539988], [-3.67762017 -4.35907246 -4.58621427 -4.00975778], [-4.52484924 -4.12304192 -4.40973955 -3.99283372], [-4.74051736 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -5.04493064 -4.73832589 -4.69546997], [-4.75942004 -5.08837561 -5.14954628 -4.30485129], [-3.71046625 -3.3713502  -4.21314226 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, right, right, down, ]\n",
            "       [ right, up, right, stay, ]\n",
            "]\n",
            "episode #90 [\n",
            "       [ [0. 0. 0. 0.], [-2.01408282 -3.69090625 -1.         -4.45231747], [-4.32926561 -4.0925835  -1.90153891 -4.8843675 ], [-4.79785992 -4.43878311 -3.39740111 -4.86190855], ]\n",
            "       [ [-0.99995806 -3.99865999 -4.09410859 -4.19952126], [-1.9134732  -3.87813348 -3.93096985 -4.45809357], [-3.68757647 -3.86010954 -2.80984619 -3.60093611], [-4.13796436 -2.01256836 -4.16952435 -4.97209242], ]\n",
            "       [ [-1.95204336 -5.01868279 -4.3272615  -4.06539988], [-3.11523689 -4.35907246 -4.58621427 -4.00975778], [-4.52484924 -4.12304192 -4.40973955 -3.99283372], [-4.74051736 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -5.04493064 -4.73832589 -4.69546997], [-4.75942004 -5.08837561 -5.14954628 -4.30485129], [-3.71046625 -3.3713502  -4.21314226 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ right, up, right, stay, ]\n",
            "]\n",
            "episode #100 [\n",
            "       [ [0. 0. 0. 0.], [-1.94563313 -3.69090625 -1.         -4.45231747], [-3.35803865 -4.0925835  -1.90024623 -4.8843675 ], [-4.79785992 -4.43878311 -2.98579145 -4.86190855], ]\n",
            "       [ [-0.99998322 -3.99865999 -2.77762079 -4.19952126], [-1.90215571 -3.83348131 -3.93096985 -4.45809357], [-3.68757647 -3.86010954 -3.81813056 -3.60093611], [-4.13796436 -2.01256836 -4.16952435 -4.97209242], ]\n",
            "       [ [-1.95204336 -5.01868279 -4.3272615  -4.06539988], [-2.87937028 -4.35907246 -4.58621427 -4.00975778], [-4.52484924 -2.78921677 -4.40973955 -3.99283372], [-4.74051736 -1.         -4.11845977 -2.93621819], ]\n",
            "       [ [-4.55867121 -5.04493064 -4.73832589 -4.69546997], [-4.75942004 -5.08837561 -5.14954628 -2.86194052], [-3.71046625 -3.3713502  -4.21314226 -1.        ], [0. 0. 0. 0.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, left, ]\n",
            "       [ up, up, left, down, ]\n",
            "       [ up, up, right, down, ]\n",
            "       [ right, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw7IhLpiP2J"
      },
      "source": [
        "## Tester cells (to be ignored)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2coXnct5GI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfa6b75-5336-4e11-fa3e-3804a70e22d2"
      },
      "source": [
        "np.arange(1,15)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIiMNeIid7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
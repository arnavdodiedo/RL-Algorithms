{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ADkFcuITboaQ",
        "3sw7IhLpiP2J"
      ],
      "authorship_tag": "ABX9TyPgKTF8NGN4PO02wh3Smf8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdodiedo/RL-Algorithms/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPTWDQ1btQo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIncFsMcHDI"
      },
      "source": [
        "## Generalized policy iteration with DP, Monte Carlo, TD learning and Q learning\n",
        "#### where the initial policy is a uniform one, equal probabilites of going to each neighbouring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCe0vwSXOv6"
      },
      "source": [
        "class gridworld():\n",
        "    def __init__(self, size = 4, reward = 0, penalty_per_step = -1, epochs = 100, gamma = 0.9, epsilon = 0.1, epsilon_decay_factor = 0.1, seed = 0):\n",
        "        self.size = size # square grid world size\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.returngrid = np.zeros((size, size), dtype=np.float) # reward for all states initialised to 0        \n",
        "        self.goal_states = [[0,0],[size-1, size-1]] # goal states are the top left and bottom right corners of the grid\n",
        "        self.reward = reward # 0 reward on reaching goal state\n",
        "        self.epsilon = epsilon\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # terminate on reaching goal state OR stuck in infinite loop with 0 rewards per step in this loop\n",
        "\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # state action value matrix\n",
        "\n",
        "        self.penalty_per_step = penalty_per_step # -1 reward per movement \n",
        "        self.movements = [[-1,0], [1,0], [0,-1], [0,1]]\n",
        "        self.episodes = self.epochs = epochs # number of epochs to run policy evalution\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def reset_grid(self): # reset all agent and environment variables\n",
        "        self.returngrid = np.zeros((self.size, self.size))\n",
        "        self.epsilon = 0.1\n",
        "        self.policy = np.zeros((self.size, self.size, 4)) + 0.25 # set initial policy as naive, equal probabilities in all directions\n",
        "        self.policy[self.goal_states[0][0],self.goal_states[0][1]] = \\\n",
        "                self.policy[self.goal_states[1][0], self.goal_states[1][1]] = np.zeros(4) # set movement probability for goal state as all 0\n",
        "        self.state_action_values = np.zeros((self.size, self.size, 4)) # reset state action value matrix\n",
        "\n",
        "    def greedy_policy_update_dp(self, curr_state):  # update policy greedily, second step of generalized policy iteration (GPI)\n",
        "        if curr_state not in self.goal_states:\n",
        "            self.policy[curr_state[0]][curr_state[1]] = np.zeros(4)\n",
        "            \n",
        "            up = self.next_state(curr_state, 0)\n",
        "            up = self.returngrid[up[0]][up[1]]\n",
        "\n",
        "            down = self.next_state(curr_state, 1)\n",
        "            down = self.returngrid[down[0]][down[1]]\n",
        "            \n",
        "            left = self.next_state(curr_state, 2)\n",
        "            left = self.returngrid[left[0]][left[1]]\n",
        "            \n",
        "            right = self.next_state(curr_state, 3)\n",
        "            right = self.returngrid[right[0]][right[1]]\n",
        "\n",
        "            arr = [[up, 0], [down, 1], [left, 2], [right, 3]]            \n",
        "            arr.sort()       \n",
        "            arr.reverse()     \n",
        "\n",
        "            m = [arr[0][1]]\n",
        "            for i in range(1, 4):\n",
        "                if arr[i][0] != arr[i-1][0]: break\n",
        "                else: m.append(arr[i][1])\n",
        "            \n",
        "            value = 1/len(m)\n",
        "            \n",
        "            for i in m:\n",
        "                self.policy[curr_state[0]][curr_state[1]][i] = value\n",
        "            \n",
        "            # print(self.policy[curr_state[0]][curr_state[1]])\n",
        "\n",
        "\n",
        "    def next_state(self, curr_state, direction): # direction = 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        change = self.movements[direction]\n",
        "        new_state = [curr_state[0]+change[0], curr_state[1]+change[1]]\n",
        "\n",
        "        if(new_state[0]<0): new_state[0] = 0\n",
        "        elif(new_state[0]>self.size-1): new_state[0] = self.size-1\n",
        "        \n",
        "        if(new_state[1]<0): new_state[1] = 0\n",
        "        elif(new_state[1]>self.size-1): new_state[1] = self.size-1\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    def display_grid(self): # display the state of the grid world                \n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.returngrid[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_state_action_values(self): # display state action values\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):\n",
        "                print(self.state_action_values[i][j], end=\", \")\n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def display_policy(self): # display policy at each state, for each of the 4 actions\n",
        "        print(\"[\", end=\"\")\n",
        "        for i in range(self.size):\n",
        "            print(\"\\n       [\", end=\" \")\n",
        "            for j in range(self.size):                \n",
        "                \n",
        "                if [i,j] in self.goal_states: \n",
        "                    print(\"stay\", end=\", \")\n",
        "                    continue\n",
        "\n",
        "                probs = [[self.policy[i][j][0], 0], [self.policy[i][j][1], 1], [self.policy[i][j][2], 2], [self.policy[i][j][3], 3]]\n",
        "                probs.sort()\n",
        "                probs.reverse()\n",
        "\n",
        "                moves = [\"up\", \"down\", \"left\", \"right\"]\n",
        "                m = [probs[0][1]]\n",
        "\n",
        "                for k in range(1,4):\n",
        "                    if probs[k][0] != probs[k-1][0]: break\n",
        "                    else: m.append(probs[k][1])\n",
        "                \n",
        "                for k in range(len(m)):\n",
        "                    if k!=len(m)-1: print(moves[m[k]], end=\"&\")\n",
        "                    else: print(moves[m[k]], end=\", \")\n",
        "                \n",
        "            print(\"]\", end=\"\")\n",
        "        print(\"\\n]\")\n",
        "\n",
        "    def evaluate_current_policy_dp(self): # policy evaluation using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):                        \n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.returngrid[new_state[0], new_state[1]]) * self.policy[i][j][k]                                \n",
        "                    \n",
        "                    self.returngrid[i][j] = value\n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()                \n",
        "    \n",
        "    def gpi_dp(self): # GPI using dp\n",
        "        self.reset_grid() # start from scratch\n",
        "        for _ in range(1, 1+self.epochs):\n",
        "            # either update in self.returngrid as you traverse OR in each epoch maintain a copy of self.returngrid and use it for update at the end\n",
        "            returngrid_copy = np.copy(self.returngrid)\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):                    \n",
        "                    value = 0\n",
        "                    for k in range(4):\n",
        "                        new_state = self.next_state([i,j], k)\n",
        "                        value += (self.penalty_per_step+self.returngrid[new_state[0]][new_state[1]]) * self.policy[i][j][k]                    \n",
        "                    returngrid_copy[i,j] = value\n",
        "\n",
        "            if (self.returngrid == returngrid_copy).all():                 \n",
        "                print(\"Policy did not improve. Stopping...\")\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "                break\n",
        "            else: \n",
        "                self.returngrid = returngrid_copy\n",
        "            \n",
        "            for i in range(self.size): \n",
        "                for j in range(self.size): \n",
        "                    self.greedy_policy_update_dp([i,j])            \n",
        "\n",
        "            if (_%10==0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy -\")\n",
        "                self.display_policy()\n",
        "\n",
        "    def evaluate_current_policy_monte_carlo(self): # evaluate policy using first visit monte carlo\n",
        "        self.reset_grid() # start from scratch\n",
        "\n",
        "        number_of_visits = np.zeros((self.size, self.size))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))\n",
        "            i = k//4\n",
        "            j = k - 4*i                        \n",
        "            \n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []\n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]] == 0:\n",
        "                    number_of_visits[state[0],state[1]] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]] = 1\n",
        "                \n",
        "                path.append(state)\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "                direction = np.random.choice(range(len(self.movements)), p=self.policy[state[0]][state[1]])\n",
        "                new_state = self.next_state(state, direction)            \n",
        "                state = new_state\n",
        "                \n",
        "            state_returns = np.zeros_like(state_reward, dtype=np.float)                        \n",
        "            state_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_returns[i] = state_reward[i] + self.gamma * state_returns[i+1]\n",
        "            \n",
        "            is_visited_this_episode = np.zeros((self.size, self.size))\n",
        "\n",
        "            for p in range(len(path)):\n",
        "                if is_visited_this_episode[path[p][0]][path[p][1]] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[path[p][0]][path[p][1]] = 1\n",
        "                \n",
        "                self.returngrid[path[p][0]][path[p][1]] += (state_returns[p]-self.returngrid[path[p][0]][path[p][1]])/number_of_visits[path[p][0]][path[p][1]]\n",
        "            \n",
        "            if (_%1000 == 0):\n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_grid()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()\n",
        "    \n",
        "    def epsilon_greedy_policy_update_monte_carlo(self, curr_state): # epsilon greedy update of policy, second step of monte carlo policy improvement\n",
        "        i, j = curr_state[0], curr_state[1]\n",
        "            \n",
        "        prob = np.zeros(len(self.movements)) + self.epsilon/len(self.movements)        \n",
        "        prob[np.argmax(self.state_action_values[i][j])] = 1 - self.epsilon + self.epsilon/len(self.movements)\n",
        "\n",
        "        # prob = np.zeros(len(self.movements))\n",
        "        # prob[np.argmax(self.state_action_values[i][j])] = 1\n",
        "\n",
        "        self.policy[i][j] = prob\n",
        "        \n",
        "        # print(\"updated policy at\", curr_state, \"to\", self.policy[i][j])\n",
        "\n",
        "    def gpi_monte_carlo(self): # GPI using epsilon greedy first visit monte carlo\n",
        "        self.reset_grid()\n",
        "        number_of_visits = np.zeros((self.size, self.size, 4))\n",
        "        for _ in range(1, 1+self.episodes):\n",
        "            # print(\"epoch\", _)\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "\n",
        "            action = np.random.choice([0,1,2,3])\n",
        "            k = np.random.choice(range(1, self.size*self.size-1))            \n",
        "            i = k//4\n",
        "            j = k - 4*i                                    \n",
        "\n",
        "            state = [i, j]\n",
        "            state_reward = []                        \n",
        "            path = []            \n",
        "\n",
        "            if state in self.goal_states: continue\n",
        "\n",
        "            while state not in self.goal_states:\n",
        "                if is_visited_this_episode[state[0]][state[1]][action] == 0:\n",
        "                    number_of_visits[state[0]][state[1]][action] += 1\n",
        "                    is_visited_this_episode[state[0]][state[1]][action] = 1\n",
        "\n",
        "                # print(\"in state\", state, \"took action\", action, \"with policy\", self.policy[state[0]][state[1]])  \n",
        "                # if path.count([state, action]) > 2: break                \n",
        "\n",
        "                path.append([state, action])\n",
        "                state_reward.append(self.penalty_per_step)\n",
        "\n",
        "                pr = self.policy[state[0]][state[1]]\n",
        "                pr[np.argmax(pr)] -= .2\n",
        "                pr[np.random.choice(np.delete(np.arange(4), np.argmax(pr)))] += .1\n",
        "                pr[np.random.choice(np.delete(np.arange(4), np.argmax(pr)))] += .1\n",
        "                \n",
        "                action = np.random.choice(range(len(self.movements)), p=pr)                \n",
        "                state = self.next_state(state, action)                                \n",
        "                \n",
        "            state_action_returns = np.zeros_like(state_reward, dtype=np.float)            \n",
        "            # print(state_returns.shape, len(state_reward))\n",
        "            state_action_returns[-1] = state_reward[-1]\n",
        "\n",
        "            for i in range(len(state_reward)-2, -1, -1):\n",
        "                state_action_returns[i] = state_reward[i] + self.gamma * state_action_returns[i+1]\n",
        "                # print(state_returns[i], self.gamma * state_returns[i+1], end=\",\")\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            is_visited_this_episode = np.zeros((self.size, self.size, 4))\n",
        "            for p in range(len(path)):\n",
        "                x, y, a = path[p][0][0], path[p][0][1], path[p][1]\n",
        "\n",
        "                if is_visited_this_episode[x][y][a] == 1: continue\n",
        "                \n",
        "                is_visited_this_episode[x][y][a] = 1                \n",
        "                self.state_action_values[x][y][a] += (state_action_returns[p]-self.state_action_values[x][y][a])/number_of_visits[x][y][a]\n",
        "                self.epsilon_greedy_policy_update_monte_carlo([x,y])                \n",
        "                \n",
        "            \n",
        "            if (_%10==0): self.epsilon *= self.epsilon_decay_factor\n",
        "\n",
        "            if (_%(self.epochs/10) == 0):                \n",
        "                print(\"epoch #%d\"%(_), end=\" \")\n",
        "                self.display_state_action_values()\n",
        "                print(\"policy - \")\n",
        "                self.display_policy()            "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkFcuITboaQ"
      },
      "source": [
        "## Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gVHaw87EBs",
        "outputId": "3c02cf57-abfa-46eb-ca09-4f81c4186184"
      },
      "source": [
        "grid = gridworld(epochs=100)\n",
        "grid.gpi_dp()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy did not improve. Stopping...\n",
            "epoch #4 [\n",
            "       [ 0.0, -1.0, -2.0, -3.0, ]\n",
            "       [ -1.0, -2.0, -3.0, -2.0, ]\n",
            "       [ -2.0, -3.0, -2.0, -1.0, ]\n",
            "       [ -3.0, -2.0, -1.0, 0.0, ]\n",
            "]\n",
            "policy -\n",
            "[\n",
            "       [ stay, left, left, left&down, ]\n",
            "       [ up, left&up, right&left&down&up, down, ]\n",
            "       [ up, right&left&down&up, right&down, down, ]\n",
            "       [ right&up, right, right, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmWnDXbz-Q"
      },
      "source": [
        "## Monte Carlo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI3VPo26ZBP4",
        "outputId": "d3acb443-e56c-4c64-e068-46469285da64"
      },
      "source": [
        "grid = gridworld(size = 4, epochs=100, gamma = 0.9, epsilon=0.99, epsilon_decay_factor = 0.9, seed = 10)\n",
        "grid.gpi_monte_carlo()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #10 [\n",
            "       [ [-100. -100. -100. -100.], [-100. -100.   -1. -100.], [  -2.305  -100.       -3.0745 -100.    ], [  -8.33228183 -100.         -100.           -6.92115386], ]\n",
            "       [ [  -4.68559 -100.        -2.2195    -4.0951 ], [-100.         -100.           -5.91465175   -1.9       ], [  -8.82434695   -2.71         -7.3169194  -100.        ], [  -8.49905365   -7.94108868 -100.           -7.58522746], ]\n",
            "       [ [  -6.5132156   -4.0951      -5.6953279 -100.       ], [-100.           -9.92144833   -6.5132156    -4.78239755], [-6.92239222 -8.27643125 -5.6953279  -6.12579511], [-100.           -3.93094702 -100.           -8.78423345], ]\n",
            "       [ [  -7.45813417   -6.74940209   -8.21899123 -100.        ], [-100.           -4.84090679   -7.54200153   -5.28489911], [-100.           -7.79378763 -100.           -3.0981021 ], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, up, right, ]\n",
            "       [ left, right, down, right, ]\n",
            "       [ down, right, left, down, ]\n",
            "       [ down, down, right, stay, ]\n",
            "]\n",
            "epoch #20 [\n",
            "       [ [-100. -100. -100. -100.], [  -4.22906709 -100.           -1.         -100.        ], [  -3.57859837 -100.           -3.0745       -6.5132156 ], [  -8.33228183 -100.         -100.           -6.92115386], ]\n",
            "       [ [  -4.062295   -100.           -3.98330836   -4.0951    ], [  -8.18640651 -100.           -5.91465175   -5.27457414], [-7.63200684 -4.50712837 -6.2429796  -8.1056872 ], [  -8.49905365   -7.94108868 -100.           -6.38784321], ]\n",
            "       [ [  -7.21700438   -4.0951       -8.65260454 -100.        ], [-9.81650619 -9.74876837 -7.2439713  -5.8352946 ], [-7.10097287 -6.27534006 -5.6953279  -6.97210137], [-100.           -2.95396468 -100.           -3.02820701], ]\n",
            "       [ [  -7.45813417   -7.57987072   -8.41011986 -100.        ], [-100.           -6.98410883   -8.09544621   -7.00353819], [-100.           -7.76663201   -1.           -3.98153066], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, left, right, ]\n",
            "       [ left, right, down, right, ]\n",
            "       [ down, right, left, down, ]\n",
            "       [ up, down, left, stay, ]\n",
            "]\n",
            "epoch #30 [\n",
            "       [ [-100. -100. -100. -100.], [  -4.22906709 -100.           -1.           -1.        ], [-3.57859837 -1.9        -5.08756873 -6.5132156 ], [  -8.49190203 -100.           -9.35389181   -7.70918262], ]\n",
            "       [ [  -6.70756795 -100.           -5.19791491   -4.0951    ], [  -8.18640651 -100.           -5.91465175   -7.24023057], [-7.86543184 -5.63415355 -5.72631196 -8.18121874], [  -8.09812375   -7.55839666 -100.           -7.00378333], ]\n",
            "       [ [  -7.89550857   -4.0951       -8.9826488  -100.        ], [-6.77924931 -9.738475   -7.97638466 -6.83951874], [-7.25396988 -6.51650933 -6.80196163 -7.14322048], [  -9.35389181   -4.49390006 -100.           -3.74068103], ]\n",
            "       [ [  -7.45813417   -7.89010225   -8.45122772 -100.        ], [-100.           -7.50908784   -6.85962167   -7.1440204 ], [-100.           -6.49500627   -3.34766395   -4.79254999], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, down, right, ]\n",
            "       [ right, left, down, right, ]\n",
            "       [ down, up, down, right, ]\n",
            "       [ up, left, left, stay, ]\n",
            "]\n",
            "epoch #40 [\n",
            "       [ [-100. -100. -100. -100.], [  -2.61453354 -100.           -1.           -1.        ], [-3.57859837 -1.9        -4.29067655 -6.5132156 ], [  -8.49190203 -100.           -9.35389181   -6.8551461 ], ]\n",
            "       [ [  -6.43545898 -100.           -5.45860985   -6.75680131], [  -6.72698484 -100.           -7.0610651    -6.41434344], [-7.86543184 -6.3781012  -6.34167707 -6.99566406], [  -8.48775072   -7.55839666 -100.           -7.33062036], ]\n",
            "       [ [  -8.09746031   -6.34639596   -8.60355901 -100.        ], [-6.5367916  -8.08763234 -7.97638466 -7.4786895 ], [-7.25396988 -6.71405166 -6.80196163 -7.30742762], [  -9.35389181   -5.51033773 -100.           -4.24860495], ]\n",
            "       [ [-5.44856709 -7.76345993 -8.31275547 -8.64914828], [-100.           -7.74202137   -6.85494196   -6.79961721], [-100.           -6.54771843   -3.34766395   -4.74062564], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, left, down, right, ]\n",
            "       [ left, right, left, right, ]\n",
            "       [ down, up, down, right, ]\n",
            "       [ up, right, left, stay, ]\n",
            "]\n",
            "epoch #50 [\n",
            "       [ [-100. -100. -100. -100.], [  -2.0763557  -100.           -2.64046711   -1.        ], [-4.76701924 -1.9        -4.36965924 -7.81346089], [  -7.99723042 -100.           -9.35389181   -6.7335876 ], ]\n",
            "       [ [  -6.30871772 -100.           -4.93891994   -5.40786754], [  -6.39636368 -100.           -6.35499992   -5.72066601], [-7.86543184 -6.81762252 -5.82286606 -6.68682428], [  -8.48775072   -7.55839666 -100.           -7.37303152], ]\n",
            "       [ [  -8.2266117    -6.66445364   -8.13192284 -100.        ], [-6.67713617 -7.21862621 -7.69982084 -7.56222579], [-7.03130674 -6.44202674 -7.14571387 -7.25224548], [  -9.35389181   -5.98066175 -100.           -3.90666138], ]\n",
            "       [ [-5.44856709 -7.71383274 -8.36293215 -8.64914828], [-3.439      -7.77883577 -6.67488662 -6.71001547], [-100.           -6.31304562   -3.34766395   -4.89508708], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right, down, right, ]\n",
            "       [ left, right, left, right, ]\n",
            "       [ down, up, down, right, ]\n",
            "       [ up, up, left, stay, ]\n",
            "]\n",
            "epoch #60 [\n",
            "       [ [-100. -100. -100. -100.], [  -3.83083174 -100.           -3.66442817   -1.        ], [-4.50141539 -1.9        -5.32390313 -6.0938654 ], [-8.22052557 -7.17570464 -9.35389181 -7.03483109], ]\n",
            "       [ [  -5.54502946 -100.           -4.80161169   -5.40786754], [  -6.55125534 -100.           -6.19046495   -5.67119348], [-6.91464763 -6.16301231 -5.86073219 -6.39086196], [  -8.48775072   -7.87370838 -100.           -7.0285707 ], ]\n",
            "       [ [-8.02074397 -5.96753832 -8.13308107 -7.71232075], [-6.92879682 -6.87476067 -7.82258382 -7.16966119], [-7.03130674 -6.16626738 -7.58388089 -7.08053917], [-9.35389181 -5.51342435 -9.7972444  -3.77288396], ]\n",
            "       [ [-5.44856709 -7.68669335 -8.33303639 -8.64914828], [-3.439      -7.41227955 -6.85227677 -6.75841238], [-100.           -6.05582743   -3.34766395   -4.89508708], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right, down, right, ]\n",
            "       [ left, right, left, right, ]\n",
            "       [ down, down, down, right, ]\n",
            "       [ up, up, left, stay, ]\n",
            "]\n",
            "epoch #70 [\n",
            "       [ [-100. -100. -100. -100.], [  -3.83083174 -100.           -4.44289646   -1.        ], [-5.72024205 -5.81093581 -5.67678771 -6.38600177], [-8.09346034 -7.17570464 -9.35389181 -7.3209862 ], ]\n",
            "       [ [  -4.94850929 -100.           -4.83154561   -5.40786754], [  -7.14914409 -100.           -6.86560273   -5.8201052 ], [-7.17769811 -6.70531604 -6.02609672 -6.98233829], [  -8.48775072   -7.81567606 -100.           -7.09248485], ]\n",
            "       [ [-8.10561707 -6.19885511 -7.81227314 -7.71232075], [-7.53736761 -7.0968143  -7.80721227 -7.29163835], [-7.03933024 -6.58891336 -7.58388089 -7.25733082], [-9.35389181 -5.16299631 -6.2536222  -3.50573663], ]\n",
            "       [ [-6.63745442 -7.85562685 -8.45156867 -8.64914828], [-3.439      -7.39313075 -7.18099497 -6.76948238], [-100.           -6.48442232   -3.34766395   -5.23851799], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right, left, down, ]\n",
            "       [ left, right, left, right, ]\n",
            "       [ down, down, down, right, ]\n",
            "       [ up, up, left, stay, ]\n",
            "]\n",
            "epoch #80 [\n",
            "       [ [-100. -100. -100. -100.], [  -4.22963154 -100.           -5.83121709   -3.23744025], [-6.45127305 -6.75700663 -6.32935846 -6.98453788], [-7.90544793 -7.17570464 -6.72449591 -7.42011017], ]\n",
            "       [ [  -4.37242888 -100.           -4.68417847   -5.40786754], [-7.2380766  -7.45813417 -6.75949753 -5.8201052 ], [-7.17769811 -6.77385577 -6.15628194 -6.96293718], [-8.45665694 -6.96420739 -8.14697981 -6.90731262], ]\n",
            "       [ [-8.05482339 -6.19885511 -7.82935636 -7.71232075], [-7.66885321 -7.13235589 -7.90021142 -7.51005137], [-7.29966591 -6.26229558 -7.58388089 -7.36582029], [-9.35389181 -4.68525201 -6.2536222  -3.34516297], ]\n",
            "       [ [-7.47694888 -7.86793907 -8.56237114 -8.64914828], [-3.439      -7.57523017 -7.18817131 -7.16273095], [-100.           -6.29185295   -5.53601609   -5.25535464], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right, left, left, ]\n",
            "       [ up, right, left, right, ]\n",
            "       [ down, down, down, right, ]\n",
            "       [ up, up, right, stay, ]\n",
            "]\n",
            "epoch #90 [\n",
            "       [ [-100. -100. -100. -100.], [  -4.61365913 -100.           -5.67006485   -3.23744025], [-6.48008919 -5.92750497 -6.39446201 -6.62089588], [-7.86478305 -7.17570464 -6.72449591 -7.38289576], ]\n",
            "       [ [  -3.91255221 -100.           -4.84823506   -5.40786754], [-7.21081231 -8.12118381 -6.87145418 -6.06588397], [-7.02291081 -6.94488367 -6.44797775 -7.07168651], [-8.45665694 -6.96420739 -8.14697981 -7.06522217], ]\n",
            "       [ [-7.76587497 -6.19885511 -7.64642484 -7.71232075], [-7.8374721  -7.33934765 -7.7068192  -7.24338185], [-7.03808697 -6.31060929 -6.77189791 -7.55398966], [-9.35389181 -4.68527801 -6.2536222  -3.38087425], ]\n",
            "       [ [-7.28420222 -7.63712468 -8.23684567 -8.64914828], [-6.54781581 -7.76563514 -7.29352291 -7.29016073], [-100.           -6.14583308   -4.40201207   -5.10861827], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right, down, left, ]\n",
            "       [ up, right, left, down, ]\n",
            "       [ down, right, down, right, ]\n",
            "       [ up, up, left, stay, ]\n",
            "]\n",
            "epoch #100 [\n",
            "       [ [-100. -100. -100. -100.], [  -4.84221175 -100.           -5.84381288   -3.23744025], [-6.62774582 -5.92750497 -6.50548736 -6.50679693], [-7.78120896 -7.17570464 -6.72449591 -7.43065102], ]\n",
            "       [ [  -3.66983953 -100.           -4.58283953   -4.30590066], [-7.33872    -7.11272778 -6.88904788 -6.06588397], [-7.05661165 -6.91439835 -6.85268583 -7.19125905], [-7.76015304 -6.96420739 -9.03421407 -7.27356651], ]\n",
            "       [ [-7.53737069 -6.19885511 -7.41656785 -7.71232075], [-7.57071367 -7.43037972 -7.76629238 -7.04315123], [-6.82787362 -6.51443654 -6.63732666 -7.23327642], [-9.35389181 -5.00104749 -6.2536222  -3.46971477], ]\n",
            "       [ [-7.28420222 -7.51394657 -7.940961   -8.64914828], [-6.54781581 -7.74088634 -7.23665489 -7.29016073], [-7.94108868 -6.13847514 -4.40201207 -5.11561264], [-100. -100. -100. -100.], ]\n",
            "]\n",
            "policy - \n",
            "[\n",
            "       [ stay, right, down, left, ]\n",
            "       [ up, right, left, down, ]\n",
            "       [ down, right, down, right, ]\n",
            "       [ up, up, left, stay, ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw7IhLpiP2J"
      },
      "source": [
        "## Tester cells (to be ignored)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2coXnct5GI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIiMNeIid7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}